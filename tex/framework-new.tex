\section{\SYSTEM{}: Learning Control}
\label{sec:framework}

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/Overview.pdf}
  \caption{\SYSTEM{} overview.}
  \label{fig:overview}
\end{figure}


\figref{fig:overview} shows \SYSTEM{}'s approach of splitting resource
management into separate learning and control tasks and then composing
these individual solutions.  When a new application enters the system,
an adaptive control system allocates resources using a generic model
and records latency and power.  The recorded values are sent to a
learner, which predicts the application's latency and power in all
other resource configurations. The learner extracts those that are
predicted to be Pareto-optimal and packages them in a data structure:
the performance hash table (PHT).  The PHT and the estimated variance
are sent to the controller, which sets its pole and selects an energy
minimal resource configuration with formal guarantees of convergence
to the desired latency.  \SYSTEM{}'s only user-specified parameter
is the latency requirement.

\begin{figure}
  \includegraphics[width=\columnwidth]{figures/Timeline.pdf}
  \caption{Temporal relationship of learning and control.}
  \label{fig:timeline}
  \vskip -1.5em
\end{figure}

\figref{fig:timeline} illustrates the asynchronous interaction between
\SYSTEM{}'s learner and controller. The controller starts---using a
conservative, generic speedup model---when a new application launches.
The controller sends the learner the application's name and device
type (message 1, \figref{fig:timeline}).  The learner determines how
many samples are needed for an accurate prediction and sends this
number to the controller (message 2).  The controller takes these
samples and sends the latency and power of each measured
configuration to the learner (message 3).  The learner may require
time to make predictions; so, the controller does not wait, but
continues with the conservative model.  Once the learner predicts the
optimal configurations, it sends that data and the variance estimate
to the controller (message 4), which uses the learned model from then
on.

\figref{fig:timeline} shows several key points about the relationship
between learning and control.  First, the controller never waits for
the learner: it uses a conservative, less-efficient control
specification until the learner produces application-specific
predictions.  Second, the controller does not continuously communicate
with the learner---this interaction happens once at application
launch.  Third, if the learner crashed, the controller defaults to the
generic adaptive control system.  If the learner crashed after sending
its predictions, the controller does not need to know.  Finally, the
learner and controller have a clearly defined interface, so they can
be run in separate processes or physically separate devices.

We first describe adaptive control.  We then generalize this approach,
separating out parameters to be learned.  Next, we discuss the class
of learners that work with \SYSTEM{}.  Finally, we formally analyze
\SYSTEM{}'s guarantees.


\subsection{Traditional Control for Computing}
A multiple-input, multiple-output (MIMO) controller manages multiple
resources to meet multiple goals.  The inputs are measurements, \eg{}
latency.  The outputs are the resource settings to be used at a
particular time, \eg{} an allocation of big and LITTLE cores and a
clockspeed for each.

These difference equations describe a generic MIMO controller for
allocating $n$ resources to meet $m$ goals at time $t$:\footnote{We
  assume discrete time, and thus, use difference equations rather than
  differential equations that would be used for continuous systems.}
\begin{equation}
\begin{aligned}
&\x(t+1) &=& \mathbf{A} \cdot \x(t)& + \mathbf{B} \cdot \mathbf{u}(t)\\
&\y(t)   &=& \mathbf{C} \cdot \x(t)&,
\end{aligned}
\label{eqn:system:mimo}
\end{equation}
where $\x \in \R^{q}$ is the controller's \emph{state}, an abstraction
of the relationship between resources and goals; $q$ is the
controller's \emph{degree}, or complexity of its internal state.
$\mathbf{u}(t) \in \R^n$ represents the current resource
\emph{configuration}; \ie{} the $i$th vector element is the amount of
resource $i$ allocated at time $t$.  $\y(t) \in \R^{m}$ represents the
value of the goal dimensions at time $t$. The matrices $\mathbf{A} \in
\R^{q \times q}$ and $\mathbf{B} \in \R^{q \times n}$ relate the
resource configuration to the controller state.  The matrix
$\mathbf{C} \in \R^{m \times q}$ relates the controller state to the
expected behavior.  This control definition does not assume the states
or the resources are independent, but it does assume a linear
relationship.

For example, in our ARM big.LITTLE system there are four resources:
the number of big cores, the number of LITTLE cores, and the speeds
for each of the big and LITTLE cores.  There is also a single goal:
latency.  Thus, in this example, $n=4$ and $m=1$. The vector
$\mathbf{u}(t)$ has four elements representing the resource allocation
at time $t$. $q$ is the number of variables in the controller's state
which can vary between 1 to $n$.  The matrices $\mathbf{A}$,
$\mathbf{B}$, and $\mathbf{C}$ capture the linear relationship between
the control state $\x$, the resource usage $\mathbf{u}$, and the
measured behavior.  In this example, we know there is a non-linear
relationship between the resources.  We overcome this difficulty by
tuning the matrices at each time step---approximating the non-linear
system through a series of changing linear formulations.  This
approximation is a form of \emph{adaptive} or \emph{self-tuning}
control \cite{HandbookControl}.  Such adaptive controllers provide
formal guarantees that they will converge to the desired latency
even in the face of non-linearities, but they still assume convexity.

This controller has two major drawbacks.  First, it requires matrix
computation, so its overhead scales poorly in the number of resources
and in the number of goals \cite{Hellerstein2004a,METE}.  Second, the
adaptive mechanisms require users to specify both (1) starting values
of the matrices $\mathbf{A}$, $\mathbf{B}$, and $\mathbf{C}$ and (2)
the method for updating these matrices to account for any
non-convexity in the relationship between resources and latency
\cite{POET,METE,ControlWare,HandbookControl}.  Therefore, typically
100s to 1000s of samples are taken at design time to ensure that the
starting matrices are sufficient to ensure convergence
\cite{FSE2015,sysid,josep-isca2016}.

\subsection{\SYSTEM{} Control System}
To overcome the above issues, \SYSTEM{} abstracts the controller of
\eqnref{system:mimo} and factors out those parameters to be learned.
Specifically, \SYSTEM{} takes three steps to transform a standard
control system into one that works without prior knowledge of the
application to be controlled:
\begin{enumerate}[leftmargin=1em]
\item controlling \emph{speedup} (which is an abstraction of latency) rather than resources;
\item turning speedup into a minimal energy \emph{resource schedule};
\item and exploiting the \emph{problem structure} to solve this
  scheduling problem in constant time.
\end{enumerate}
These steps assume a separate learner has produced predictions of how
resource usage affects latency and power.  The result is that
\SYSTEM{}'s controller runs in constant time without requiring any
user-specified parameters.


% We first describe our formulation for controlling speedup and then
% converting that speedup into resource allocations.

\subsubsection{Controlling Speedup}
\SYSTEM{} converts \eqnref{system:mimo} into a single-input
(latency), single-output (speedup) controlling using $\mathbf{A} =
0$, $\mathbf{B} = b(t)$, $\mathbf{C} = 1$,$\mathbf{u}= speedup$, and
$y = perf$; where $b(t)$ is a time-varying parameter representing the
application's \emph{base speed}---the speed when all resources are
available---and $perf$ is the measured latency. Using these
substitutions, we eliminate $\x$ from \eqnref{system:mimo} to relate
speedup to latency:
\begin{equation}
  lat(t) = 1/(b(t) \cdot speedup(t-1)) \label{eqn:speedup}
\end{equation}
While $b(t)$ is application-specific.  \SYSTEM{} assumes base speed is
time-variant as applications will transition through phases and it
estimates this value online using the standard technique of Kalman
filter estimation \cite{welch2006kalman}. 


\SYSTEM{} must eliminate the error between the target latency and the goal: $ error(t) = goal - 1/lat(t)$.
Given \eqnref{speedup}, \SYSTEM{} uses the integral control law
\cite{Hellerstein2004a}:
\begin{eqnarray}
  % error(t) &=& goal - perf(t) \label{eqn:speedup-error} \\
  % speedup(t) &=& speedup(t-1) - \frac{error(t)}{b}
  speedup(t) &=& speedup(t-1) - \frac{1 - \rho(t)}{b(t)}.error(t)
  \label{eqn:speedup-control}
\end{eqnarray}
which states that the speedup at time $t$ is a function of the
previous speedup, the error at time $t$, the base speed $b(t)$, and
the controller's \emph{pole}, $\rho(t)$.  Standard control techniques
statically determine the pole and the base speed, but \SYSTEM{}
\emph{dynamically sets the pole and base speed to account for error in
  the learner's predictions---an essential modification for providing
  formal guarantees of the combined control and learning systems.}
For stable control, \SYSTEM{} ensures $0 \le \rho(t) < 1$. Small
values of $\rho(t)$ eliminate error quickly, but make the controller
more sensitive to the learner's inaccuracies.  Larger $\rho(t)$ makes
the system more robust at the cost of increased convergence time.
\secref{guarantees} describes how \SYSTEM{} sets the pole, but we
first address converting speedup into a resource allocation.

\subsubsection{Converting Speedup to Resource Schedules}
\SYSTEM{} must map \eqnref{speedup-control}'s speedup into a resource
allocation.  On our example big.LITTLE architecture an allocation
includes big and LITTLE cores as well as a speed for both.  The
primary challenge is that speedups in real systems are discrete
non-linear functions of resource usage, while \eqnref{speedup-control}
is a continuous linear function.  We bridge this divide by assigning
time to resource allocations such that the average speedup over a
control interval is that produced by \eqnref{speedup-control}.

The assignment of time to resource configurations is a
\emph{schedule}; \eg{} spending 10 ms on the LITTLE cores at 0.6 GHz
and then 15 ms on the big cores at 1 GHz. Typically many schedules can
deliver a particular speedup and \SYSTEM{} must find one with minimal
energy.  Given a time interval $T$, the $speedup(t)$ from
\eqnref{speedup-control}, and $C$ different resource configurations,
\SYSTEM{} solves:
\begin{eqnarray}
  \minimize_{\mathbf{\tau} \in \R^{C}} && \sum_{c=0}^{C-1} \tau_c \cdot p_c \label{eqn:power}  \\
  \st %&& \nonumber\\
  && \sum_{c=0}^{C-1} \tau_c \cdot s_c =  speedup(t)T \label{eqn:work} \\
  && \sum_{c=0}^{C-1} \tau_c =  T \label{eqn:deadline} \\
  && 0 \le \tau_c \le T, \qquad \forall c \in \{0,\ldots,C-1\} \label{eqn:time}
\end{eqnarray}
where $p_c$ and $s_c$ are configuration $c$'s estimated
\emph{powerup}---analogous to speedup---and speedup; $\tau_c$ is the
time to spend in configuration $c$.  \eqnref{power} is the objective:
minimizing energy (power times time).  \eqnref{work} states that the
average speedup must be maintained, while \eqnref{deadline} requires
the time to be fully utilized.  \eqnref{time} simply avoids negative
time.

\subsection{Exploiting Problem Structure for Fast Solutions}
%\SYSTEM{} solves \eqnrref{power}{time} on the local device (see
%\figref{fig:timeline}, so the solution must be efficient.  
By encoding the learner's predictions in the performance hash table
(PHT), \SYSTEM{} solves \eqnrref{power}{time} in constant time.

Kim et al. analyze the problem of minimizing energy while meeting a
latency constraint and observe that there must be an optimal
solution with the following properties \cite{kim-cpsna}:
\begin{itemize}[leftmargin=1em]
\item At most two of $\tau_c$ are non-zero, meaning that at most two
  configurations will be used in any time interval.
\item If you chart the configurations in the power and speedup
  tradeoff space (\eg{} the top half of \figref{fig:pht}) the two
  configurations with non-zero $\tau_c$ lie on the lower convex hull
  of the points in that space.
\item The two configurations with non-zero $\tau_c$ are adjacent on
  the convex hull: one above the constraint and one below.
\end{itemize}
%\SYSTEM{} uses these two facts to construct a constant time algorithm
%for finding the optimal solution online.

\begin{figure}
\includegraphics[width=\columnwidth]{figures/performance-hash-table.pdf}
\caption{Data structure to efficiently convert required speedup into a
  resource configuration.}
  \label{fig:pht}
\end{figure}

The PHT (shown in \figref{fig:pht}) provides constant time access to
the lower convex hull.  It consists of two arrays: the first being
pointers into the second: a configuration array, which stores resource
configurations the learner estimates to be on the lower convex hull
sorted by speedup.  Recall speedups are computed relative to the base
speed, which uses all resources.  The largest estimated speedup is
therefore 1.  The first array of pointers has a \emph{resolution}
indicating how many decimal points of precision it captures and it is
indexed by speedup.  The example in \figref{fig:pht} has a resolution
of $0.1$.  Each pointer in the first array points to the configuration
in the second array that has the largest speedup less than or equal to
the index.

\SYSTEM{} computes $speedup(t)$ and uses the PHT to convert speedup
into two configurations: $hi$ and $lo$.  To find the $hi$
configuration, \SYSTEM{} clamps the desired speedup to the largest
index lower than $speedup(t)$, indexes into the configuration array,
and then walks forward until it finds the first configuration with
speedup higher than $speedup(t)$.  To find $lo$, it clamps the desired
speedup to the smallest index higher than $speedup(t)$, indexes into
the configuration array, and then walks backwards until it finds the
configuration with the largest speedup less than $speedup(t)$.

For example, consider the PHT in \figref{fig:pht} and a $speedup(t) =
.65$.  To find $hi$, \SYSTEM{} indexes at .6 and walks up to find
$c=2$ with $s_c=.8$, setting $hi = 2$.  To find $lo$, \SYSTEM{}
indexes the table at .7 and walks backward to find $c=1$ with
$s_c=.2$, setting $lo = 1$.

\SYSTEM{} sets $\tau_{hi}$ and $\tau_{lo}$ by solving:
\begin{eqnarray}
  T &=& \tau_{hi} + \tau_{lo}    \label{eqn:s1} \\
  speedup(t) &=& \frac{s_{hi} \cdot \tau_{hi} + s_{lo} \cdot \tau_{lo}}{T} \label{eqn:s2}
\end{eqnarray}
where the controller provides $speedup(t)$ and the learner predicts
$s_c$.  By solving \eqnsref{s1}{s2}, \SYSTEM{} has turned the
controller's speedup into a resource schedule using predictions stored
in the PHT.

\subsection{\SYSTEM{} Learning Algorithms}
The previous subsection describes a general control system, which can
be customized with a number of different learning methods.  The
requirements on the learner are that it must produce 1) predictions of
each resource configuration's speedup and powerup and 2) estimate of
its own variance $\sigma^2$.  This section describes the general class
of learning mechanisms that meet these requirements.

We refer to application-specific predictors as \emph{online} because
they work for the current application and do not incorporate knowledge
of other applications.  We refer to general predictors as
\emph{offline} as they use prior observations of other applications to
predict the behavior of a new application. A third class of
\emph{transfer learning} combines information from the previously seen
applications and current application to model the future behavior of
the current application \cite{pan2010survey}. Transfer learning
produces highly accurate models since it augments online data with
offline information from other applications.  \SYSTEM{} uses transfer
learners because \SYSTEM{}'s separation of learning and control makes
it easy to incorporate data from other applications---the learner in
\figref{fig:timeline} can simply aggregate data from multiple
controllers. We describe two examples of appropriate transfer learning
algorithms.

\paragraph{Netflix Algorithm:}
The Netflix problem is a famous challenge posted by Netflix to predict
users' movie preferences. The challenge was won by realizing that if 2
users both like some movies, they might have similar taste in other
movies \cite{netflix}. This approach allows learners to borrow large
amounts of data from other applications to answer questions about a
new application.  One formulation of this problem is to assume the
matrix of resource-vs-speedup is low-rank and solve the problem
using mathematical optimization techniques.  The Netflix approach has
been used to predict application response to heterogeneous resources
in data centers \cite{Paragon,quasar}.

\paragraph{ Bayesian Predictors:} A hierarchical Bayesian model (HBM)
provides a statistically sound framework for learning across
applications and devices
\cite{gelman2013bayesian,morris1983parametric}.  In the HBM, each
application has its own model, allowing specificity, but these models
are conditionally dependent on some underlying probability
distribution with a hidden mean and co-variance.  In practice, an HBM
predicts behavior for a new application using a small number of
observations and combining those with the large number of observations
of other applications.  Rather than over-generalizing, the HBM uses
only similar applications to predict new application behavior.  The
HBM's accuracy increases as more applications are observed because
increasingly diverse behaviors are represented in the pool of prior
knowledge \cite{LEO}.  Of course, the computational complexity of
learning also increases with increasing applications.


\subsection{Formal Analysis}
\label{sec:guarantees}
\paragraph{Control System Complexity}

\SYSTEM{}'s control system (see Algorithm \ref{alg:gcs}) runs on the
local device along with the application under control, so its overhead
must be minimal.  In fact, each controller invocation is $O(1)$ .  The
only parts that are not obviously constant time are the PHT lookups.
Provided the PHT resolution is sufficiently high to avoid collisions,
then each PHT lookup requires constant time.
\begin{algorithm}[t]
\caption{\SYSTEM{}'s runtime control algorithm.}
\label{alg:gcs}
\begin{algorithmic}
%\REQUIRE Initialize the controller with a generic prediction of speedup and powerup. Send power and performance samples to learner and receive a PHT.
\WHILE{$True$}
    \STATE    Measure application latency 
    \STATE    Compute required speedup (Equation \eqref{eqn:speedup})
    \STATE    Lookup $s_{hi}$ and $s_{lo}$ with PHT
    \STATE    Compute $\tau_{hi}$ and $\tau_{lo}$ (Equations \ref{eqn:s1} \& \ref{eqn:s2})
    \STATE    Configure to system to $hi$ $\&$ sleep $\tau_{hi}$.
    \STATE    Configure to $lo$ $\&$ sleep $\tau_{lo}$.
\ENDWHILE
\vskip -1.5em
\end{algorithmic}
\end{algorithm}

\paragraph{Control Theoretic Formal Guarantees}

The controller's pole $\rho(t)$ is critical to providing control
theoretic guarantees in the presence of learned---rather than directly
measured---data.  \SYSTEM{} requires any learner estimate not only
speedup and powerup, but also the variance $\sigma$.  \SYSTEM{} uses
this information to derive a lower bound for the pole which guarantees
probabilistic convergence to the desired latency. Specifically, we
prove that with probability 99.7\% \SYSTEM{} converges to the desired
latency if the pole is
$$\Floor{1- \Floor{max(\hat{s})/(min(\hat{s}) - 3\sigma)}_0}_0 \leq \rho(t)
< 1,$$ where $\Floor{x}_0 = \max(x,0)$ and $\hat{s}$ is the estimated
speedup. See appendix A for the proof. Users who need higher
confidence can set the scalar multiplier on $\sigma$ higher; \eg{}
using $6$ provides a 99.99966\% probability of convergence.

Thus we provide a lower-bound on the value of $\rho(t)$ required for
confidence that \SYSTEM{} converges to the desired latency.  This
pole value only considers latency, and not energy efficiency.  In
practice, we find it better to use a higher pole based on the
\emph{uncertainty} between the controller's observed energy efficiency
and that predicted by the learner.  We follow prior work
\cite{Tokic2010} in quantifying uncertainty as $\beta(t)$, and setting
the pole based on this uncertainty:
\begin{equation}
  \begin{array}{rcl}
    \beta(t) &=&  \text{exp}{\left(- \left( \left|   \frac{\bar{s}(t)}{\bar{p}(t)}  -\frac{ \hat{s}(t)}{\hat{p}_(t)} \right| \right) /5\right)} \\
    \rho(t) &=& \frac{1-\beta(t)}{1+\beta(t)} 
  \end{array}
  \label{eqn:uncer}
\end{equation}
where $\bar{s}$ and $\bar{p}$ are the measured values of speedup and
powerup and $\hat{s}$ and $\hat{p}$ are the estimated values from the
learner.  This measure of uncertainty captures both power and
latency.  We find that it is generally higher than the pole value
given by our lower bound, so in practice \SYSTEM{} sets the pole
dynamically to be the higher of the two values and \SYSTEM{} makes
spot updates to the estimated speedup and power based on its
observations.
