\begin{abstract}

  Many modern computing systems must provide reliable latency with
  minimal energy.  Two central challenges arise when allocating system
  resources to meet these conflicting goals: (1)
  \emph{complexity}---modern hardware exposes diverse resources with
  complicated interactions---and (2) \emph{dynamics}---latency must be
  maintained despite unpredictable changes in operating environment or
  input.  Machine learning accurately models the latency of complex,
  interacting resources, but does not address system dynamics; control
  theory adjusts to dynamic changes, but struggles with complex
  resource interaction. We therefore propose \SYSTEM{}, a resource
  manager that learns key control parameters to meet latency
  requirements with minimal energy in complex, dynamic environments.
  \SYSTEM{} breaks resource allocation into two sub-tasks: learning
  how interacting resources affect speedup, and controlling speedup to
  meet latency requirements with minimal energy. \SYSTEM{} defines a
  general control system---whose parameters are customized by a
  learning framework---while maintaining control-theoretic formal
  guarantees that the latency goal will be met. We test \SYSTEM{}'s
  ability to deliver reliable latency on heterogeneous ARM big.LITTLE
  architectures in both single and multi-application scenarios.
  Compared to state-of-the-art learning and control solutions,
  \SYSTEM{} reduces deadline misses by $60\%$ while reducing energy
  consumption by $13\%$.
  



  \PUNT{ 

    includes (1) a control abstraction where key parameters are
    automatically estimated by a noisy learning mechanism, (2) an
    efficient implementation that applies the learned models in
    constant time, and (3) formal guarantees that the system converges
    to the desired performance without prior knowledge of application
    behavior

Mobile systems must deliver performance to interactive
    applications while simultaneously conserving resources to extend
    battery life.  There are two central challenges to meeting these
    conflicting goals: (1) the complicated optimization spaces arising
    from hardware heterogeneity and (2) dynamic changes in application
    behavior and resource availability.  Machine learning techniques
    handle complicated optimization spaces, but do not incorporate
    models of system dynamics; control theory provides formal
    guarantees of dynamic behavior, but struggles with non-linear
    system models.  In this paper, we propose \SYSTEM{}, a combination
    of learning and control techniques to meet performance
    requirements on heterogeneous devices in unpredictable
    environments.  \SYSTEM{} combines a hierarchical Bayesian model
    (HBM) with a lightweight control system (LCS).  The HBM runs
    remotely, learning customized performance/power models.  The LCS
    runs on the mobile system and tunes resource usage to meet
    performance goals.  The Performance Hash Table (PHT) is the
    interface between the two and allows the LCS to apply the learned
    models in constant time.  We test \SYSTEM{}'s ability to manage
    ARM big.LITTLE systems.  Compared to existing learning and control
    methods, \SYSTEM{} delivers more reliable performance -- only 2\%
    error compared to 4.5-5.4\% for learning and 4.7\% for control --
    and lower energy -- within 7\% of optimal on average as compared
    to 25-52\% for learning and 26\% for control. Furthermore, we
    demonstrate \SYSTEM{}'s ability to meet performance and energy
    goals in dynamic systems with phase changes and multiple
    applications running on the same system.  }



  \PUNT{ When multiple applications compete for resources, these
    numbers improve: 7\% error compared to 11-15\% for learning and
    9\% for control and improvements of 2-20\% and 3\%, respectively,
    in energy efficiency.}
\end{abstract}
