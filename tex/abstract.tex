\begin{abstract}

  
  Control systems are a proven method for allocating resources to meet
  streaming applications' performance requirements with minimal energy
  on mobile and embedded devices. Control designs, however, require
  \emph{a priori} models to estimate an application's performance and
  energy based on its resource usage -- a controller designed for one
  application and system must be redesigned for a new deployment.  We
  propose CALOREE to achieve control theoretic benefits -- \ie{}
  formal, bounded convergence to required performance in dynamic
  environments -- without \emph{a priori} models.  CALOREE combines a
  generalized control system (GCS) \PUNT{that runs on a local device}
  with a hierarchical Bayesian model (HBM)\PUNT{that runs on a remote
    server}. The HBM runs on a remote server and aggregates data from
  multiple applications and devices to produce a highly accurate model
  that it sends to the GCS to customize control for the local device
  and application. We extend standard control analysis to show that
  CALOREE provides probabilistic convergence guarantees despite having
  no prior model of the controlled application.

  We implement CALOREE using ARM big.LITTLE boards for streaming
  applications and an x86 server for the HBM.  We test in both single-
  and multi-application environments (where applications compete for
  resources).  Compared to state-of-the-art learning and control
  techniques, CALOREE consistently provides the most reliable
  performance: a worst case performance error of only 12\% compared to
  70-80\% for prior approaches.  Additionally, CALOREE provides the
  lowest energy: delivering average savings from 8-47\%.


  \PUNT{CALOREE has three main components: (1) a remote learner that
    aggregates data across devices and applications to model
    application performance and energy, (2) a lightweight control
    system that uses the learned models to customize control for a
    particular application, and (3) the interfaces that map non-convex
    learned models to the continuous linear models used by the
    controllers.

  We implement CALOREE's learning on an x86 server and stream
  processing on heterogeneous ARM big.LITTLE devices. We test in both
  \emph{stable} and \emph{unstable} environments (where available
  resources change over time) and we compare to state-of-the-art
  learning and control-based resource managers.  In all test
  scenarios, \SYSTEM{} provides the lowest error between the required
  and delivered performance. Depending on the scenario, \SYSTEM{}
  provides 12-25\% average energy reduction compared to
  state-of-the-art approaches.  Additionally, \SYSTEM{}'s worst case
  performance and energy is uniformly better than prior approaches, by
  factors as high as $2\times$.
}


\PUNT{
  Streaming sensor processing forms a foundational workload for
  embedded, mobile, and Internet-of-things. Stream processing on these
  platforms requires both reliable performance and low energy, and two
  challenges must be addressed to meet these conflicting requirements.
  The first is \emph{complexity}: hardware exposes heterogeneous
  resources which interact in complicated ways. The second is
  \emph{dynamics}: performance must keep up with the data stream
  despite unpredictable changes in operating environment.  Prior work
  shows that machine learning addresses the complexity challenge and
  control systems tackle dynamics, but streaming sensor processing on
  energy-limited devices requires that both challenges be met
  simultaneously.

  To address both complexity and dynamics, we propose \SYSTEM{}, a
  resource management system that integrates hierarchical Bayesian
  learning with a lightweight control system.  The learning framework
  runs on a remote server aggregating data from multiple applications
  and devices to estimate performance/energy models for different
  resource configurations.  The controller runs on energy-limited
  devices using the learned models to deliver required performance for
  streaming processing with minimal energy.  We test \SYSTEM{} by
  implementing its learning on an x86 server and stream processing on
  four heterogeneous ARM big.LITTLE devices. We test in both
  \emph{stable} and \emph{unstable} environments (where available
  resources change over time) and we compare to state-of-the-art
  learning and control approaches.  In all test scenarios, \SYSTEM{}
  provides the lowest error between the required and delivered
  performance. Depending on the scenario, \SYSTEM{} also provides
  12-25\% average energy reduction compared to state-of-the-art
  learning and control approaches.  Additionally, \SYSTEM{}'s worst
  case performance and energy is uniformly better than prior
  approaches.  Thus, \SYSTEM{}'s unique combination of learning and
  control is well-suited to supporting streaming sensor processing in
  embedded, mobile, and IoT platforms.
}


  \PUNT{ Mobile systems must deliver performance to interactive
    applications while simultaneously conserving resources to extend
    battery life.  There are two central challenges to meeting these
    conflicting goals: (1) the complicated optimization spaces arising
    from hardware heterogeneity and (2) dynamic changes in application
    behavior and resource availability.  Machine learning techniques
    handle complicated optimization spaces, but do not incorporate
    models of system dynamics; control theory provides formal
    guarantees of dynamic behavior, but struggles with non-linear
    system models.  In this paper, we propose \SYSTEM{}, a combination
    of learning and control techniques to meet performance
    requirements on heterogeneous devices in unpredictable
    environments.  \SYSTEM{} combines a hierarchical Bayesian model
    (HBM) with a lightweight control system (LCS).  The HBM runs
    remotely, learning customized performance/power models.  The LCS
    runs on the mobile system and tunes resource usage to meet
    performance goals.  The Performance Hash Table (PHT) is the
    interface between the two and allows the LCS to apply the learned
    models in constant time.  We test \SYSTEM{}'s ability to manage
    ARM big.LITTLE systems.  Compared to existing learning and control
    methods, \SYSTEM{} delivers more reliable performance -- only 2\%
    error compared to 4.5-5.4\% for learning and 4.7\% for control --
    and lower energy -- within 7\% of optimal on average as compared
    to 25-52\% for learning and 26\% for control. Furthermore, we
    demonstrate \SYSTEM{}'s ability to meet performance and energy
    goals in dynamic systems with phase changes and multiple
    applications running on the same system.  }



  \PUNT{ When multiple applications compete for resources, these
    numbers improve: 7\% error compared to 11-15\% for learning and
    9\% for control and improvements of 2-20\% and 3\%, respectively,
    in energy efficiency.}
\end{abstract}
