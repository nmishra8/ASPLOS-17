\section{Introduction}
% Dennard Scaling making energy essential.  Architects address energy
% by making more complicated processors which expose resources to
% software management.  For a wide range of applications, need to meet
% performance goals with minimal energy.
Large classes of computing systems---from embedded to cloud---must
deliver reliable performance with minimal energy to increase battery
life or decrease operating costs.  To address these conflicting
requirements, hardware architects have begun to expose increasingly
diverse, heterogeneous resources with an array of different
performance and energy tradeoffs.  It is then software's
responsibility to determine how to allocate these reources to ensure
performance requirements are met with minimal energy.


% Difficulties of meeting performance with minimal energy. (1)
% complexity---heterogeneous resources---and (2) dynamics---adjust to
% unforeseen changes in workload and environment.
There are two primary difficulties in determining how to allocate
heterogeneous resources.  The first is \emph{complexity}---these
resources interact in complicated ways, leading to non-convex
optimization spaces.  The second is \emph{dynamics}---performance
requirements must be met despite unpredictable disturbances; \eg{}
phases in input or changes in operating environment.  Prior work has
addressed each of these difficulties individually.

% Prior approaches addressed each of these difficulties individually.
% ML---can handle complexity.  ML advantages: can handle
% non-convexity, avoid local optima, get to true optimal solution. ML
% disadvantages: advanced techniques are expensive and no notion of
% dynamics.  Control---handles dynamics.  Control advantages: formally
% analyzable guarantees despite dynamics.  Control disadvantages:
% relies on good models---no local optima, bounded error.
A number of machine learning approaches can accurately model the
complex performance/power tradeoff spaces that arise on heterogeneous
computing systems
\cite{reddiHPCA2013,dubach2010,Bitirgen2008,Ipek,Koala,LEO,Flicker,Ponamarev}.
Such ML approaches have the advantage of being able to handle
non-convexity, avoiding local optima to find globally optimal
solutions.  Such approaches have the drawbacks, however, of being
computationally expensive and lacking support for dynamics; \ie{} when
the environment changes the expensive model building process must be
restarted.  At the same time, control theoretic solutions have been
proposed to allocate resources effciently in the presence of system
dynamics
\cite{Hellerstein2004a,Chen2011,PTRADE,POET,ControlWare,Agilos,grace2}.
These approaches provide formally analyzable guarantees that they will
deliver the required performance, but these guarnatees are subject to
accurate models and do not support non-convexity or complicated
interactions between the resources to be allocated.


% Want to combine learning and control to address both difficulties
% simultaneously.  Need an interface that allows learned models to be
% used by control system.  Challenges: (1) overhead and (2) formal
% guarantees.  
Our goal is to combine learning and control techniques to ensure
performance requirements are met with minimal energy in complex and
dynamic environments.  The challenges to combining these techniques
are (1) mitigating the overhead of the learning techniques and (2)
preserving the controller's formal guarantees in the presence of the
learned models.

% Combine learning and control through CALOREE.  Describe it.
We address these challenges with \SYSTEM{}, \footnote{\textbf{C}ontrol
  \textbf{A}nd \textbf{L}earning for \textbf{O}ptimal
  \textbf{R}esource \textbf{E}nergy \textbf{E}fficiency} a framework
for combining machine learning and control theory to build resource
management frameworks that can meet application performance
requirements with minimal energy.  The two key components of this
interface are (1) a data structure (called the performance hash table)
that allows the controller to access the learned model in constant
($O(1)$) time and (2) a confidence interval and estimated standard
deviation that allow us to provide formal guarantees that the combined
learning and control system will converge to the desired performance.
The \SYSTEM{} interface not only allows us to combine control and
learning techniques, but it allows us to do so while keeping the two
physically separate.  Such physical separation allows us to run the
computationally expensive learning engines on a remote server while
the constant time control systems run on the device to be managed.  In
addition to amortizing the cost of learning, moving it to a remote
server allows us to take advantage of learning techniques that work
across devices and applications; \ie{} those that can learn
similarities between different applications and systems.


% Implement CALOREE.  Test against state of the art learning and
% self-tuning control systems.  We find that:
To demonstrate \SYSTEM{}, we implement it with learning running on an
x86 server and the control systems working to manage heterogeneous ARM
big.LITTLE devices.  We compare \SYSTEM{} to existing,
state-of-the-art learning (including multivariate polynomial
regression, the Netflix algorithm, and a hierarchical bayesian model)
and control (including proportional-integral-derivative and adaptive,
or self-tuning) techniques.  Additionally, we compare to a naive
combination of learning and control that does not account for the
confidence interval and standard deviation of the learned model.  We
set performance goals (in terms of latency requirements) for a set of
benchmark applications and then measure the percentage of time the
requirements are violated as well as the energy for each application.
We test both \emph{single-app} environments, where on application runs
alone, and \emph{multi-app} environments where other applications
unpredictably enter the system and compete for resources.  We find
that \SYSTEM{} achieves the:


% Key contributions.
% Contributions, but I decided against bulleted llist for thsi paper
In summary, control theoretic approaches are well suited to manage
resources in dynamic environments and machine learning techniques can
produce accurate models of complex processors.  \emph{To the best of
  our knowledge, \SYSTEM{} is the first work to propose combining the
  two at runtime to ensure application performance goals without prior
  knowledge of the controlled application.}  We demonstrate this
contribution by implementing a resource manage that can meet
performance requirements on mobile/embedded processors with mininmal
energy.  The primary contribution of this p.  Additional
contributions include formal analysis showing how to incorporate
learned variance into the control theoretic guarantees and the
empirical evaluation showing the combined control and learning system
outperforms individual, state-of-the-art control or learning
solutions.


