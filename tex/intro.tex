\section{Introduction}
% Dennard Scaling making energy essential.  Architects address energy
% by making more complicated processors which expose resources to
% software management.  For a wide range of applications, need to meet
% performance goals with minimal energy.
Large classes of computing systems---from embedded to cloud---must
deliver reliable performance to users while minimizing energy to
increase battery life or decrease operating costs.  To address these
conflicting requirements, hardware architects expose diverse,
heterogeneous resources with a wide array of performance and energy
tradeoffs.  Software must allocate these resources to
guarantee performance requirements are met with minimal energy.


% Difficulties of meeting performance with minimal energy. (1)
% complexity---heterogeneous resources---and (2) dynamics---adjust to
% unforeseen changes in workload and environment.
There are two primary difficulties in determining how to allocate
heterogeneous resources.  The first is \emph{complexity}: resources
interact in complicated ways, leading to non-convex optimization
spaces.  The second is \emph{dynamics}: perfor\-mance requirements
must be met despite unpredictable disturbances; \eg{} phases in input
or changes in operating environment.  Prior work addresses each of
these difficulties individually.

% Prior approaches addressed each of these difficulties individually.
% ML---can handle complexity.  ML advantages: can handle
% non-convexity, avoid local optima, get to true optimal solution. ML
% disadvantages: advanced techniques are expensive and no notion of
% dynamics.  Control---handles dynamics.  Control advantages: formally
% analyzable guarantees despite dynamics.  Control disadvantages:
% relies on good models---no local optima, bounded error.
Existing machine learning approaches predict an application's
performance and power as a function of resource configurations
\cite{reddiHPCA2013,dubach2010,Bitirgen2008,Ipek,Koala,LEO,Flicker,Ponamarev,Paragon}.
Learning approaches handle the complexity of modern hardware, but
their predictions are not useful if the environment changes
dynamically; \eg{} when a second application enters the system.
Control theoretic approaches adjust resource usage based on the
difference between measured and expected performance
\cite{Hellerstein2004a,Chen2011,POET,ControlWare,Agilos,grace2,JouleGuard}.
Control solutions provide formal guarantees they will converge to the
desired performance despite dynamic and unpredictable interference,
but these guarantees are based on known relationships between resource
usage and application performance.  If the relationships are not known
or there is significant error between the expected behavior and the
actual behavior, the controller will not converge.

% Want to combine learning and control to address both difficulties
% simultaneously. 
Intuitively, we want to combine learning and control to get
predictable behavior in complex, dynamic systems.  The combination,
however, incurs at least two major challenges:
\begin{itemize}
\item Dividing the resource allocation problem into sub-tasks
  so that each sub-task suits the different strengths of learning and
  control, while avoiding their drawbacks.
\item Defining an interface that ensures the sub-task solutions
  coordinate with each other.
\end{itemize}

We address the first challenge by splitting resource allocation into
two tasks.  The first task is learning speedup---instead of absolute
performance---so that all unpredictable external interference can be
viewed as a change to a \emph{baseline} performance and the relative
speedup is not affected by these changes.  Learning is well suited to
predicting speedup as a function of resource usage, even when the
resources have complicated relationships.  The second task is using
control to adjust speedup based on the difference between measured and
desired performance. Since there is a simple linear relationship
between speedup and performance, it is easy to build the controller
using standard techniques and even if the measured performance is
substantially changed due to external disturbances, the controller
accounts for these dynamics.

We address the second challenge by defining an interface between
control and learning and that allows fast application of control while
maintaining control's formal guarantees.  This interface consists of
two parts.  The first part is a \emph{performance hash table} (PHT)
that stores the learned relationship between configurations and
speedup.  The controller accesses the PHT to find the resource
allocation that meets a desired speedup with minimal energy and
requires only constant time---$O(1)$---to access.  The second part of
the interface is the learned variance.  Using the ratio of minimum and
maximum speedup and the learned variance, the controller can
dynamically set its error tolerance.  This part of the interface
allows the controller to maintain formal guarantees of convergence
despite the fact that speedup is predicted by a noisy learning
mechanism, rather than directly measured in traditional control
design.

The proposed approach is a general methodology allowing a wide range
of learning techniques to be paired with its controller.
\emph{Furthermore, this approach requires no user-level parameters}.
Instead, the proposed approach tunes its internal parameters
automatically, meaning that it requires no user-level knowledge of
control or learning to apply.  So, our proposed approach not only
provide an advantage over existing individual learning and control
techniques, it allows us to explore different combinations of learning
and control to find the best.

% Implement CALOREE.  Test against state of the art learning and
% self-tuning control systems.  We find that:
We call our combination of learning and control
\SYSTEM{}.\footnote{\textbf{C}ontrol \textbf{A}nd \textbf{L}earning
  for \textbf{O}ptimal \textbf{R}esource \textbf{E}nergy
  \textbf{E}fficiency} We evaluate \SYSTEM{} by implementing the
learner on an x86 server and the controller on heterogeneous ARM
big.LITTLE devices.  We compare \SYSTEM{} to existing,
state-of-the-art learning (including polynomial regression
\cite{Koala,dubach2010}, the Netflix algorithm \cite{netflix,Paragon},
and a hierarchical Bayesian model \cite{LEO}) and control (including
proportional-integral-derivative \cite{Hellerstein2004a} and adaptive,
or self-tuning \cite{HandbookControl}) techniques.  We set performance
goals---as latency requirements---for a set of benchmark applications
and then measure both the percentage of time the requirements are
violated and the energy for each application.  We test both
\emph{single-app}---where an application runs alone---and
\emph{multi-app} environments---where other applications enter the
system and compete for resources.  \SYSTEM{} achieves the:
\begin{itemize}[leftmargin=1em]
\item \textit{Most reliable performance:}
  \begin{itemize}[leftmargin=1em]
  \item In the \emph{single-app} case, the best prior technique misses
    12\% of deadlines on average, while \SYSTEM{} misses only 5\% on
    average---reducing deadline misses by more than 58\% compared to
    prior approaches.
  \item In the \emph{multi-app} case, the best prior approach averages
    30\% deadline misses, but \SYSTEM{} misses just 5.6\% of
    deadlines---a huge improvement over prior work.
  \end{itemize}
\item \textit{Best energy savings:} We compare to an \emph{oracle}
  with a perfect model of the application, system, and future events.
  \begin{itemize}[leftmargin=1em]
  \item In the \emph{single-app} case, the best prior approach
    averages 12\% more energy consumption than the oracle, but
    \SYSTEM{} consumes only 5\% more.
  \item In the \emph{multi-app} case, the best prior approach averages
    18\% more energy than the oracle, while \SYSTEM{} consumes just
    8\% more.
  \end{itemize}
\end{itemize}

% Key contributions.
% Contributions, but I decided against bulleted llist for thsi paper
In summary, control approaches are well suited to dynamic environments
and learning techniques accurately model complex, heterogeneous
processors.  \emph{To the best of our knowledge, \SYSTEM{} is the
  first work to combine the two to ensure application
  performance---both formally and empirically---without prior
  knowledge of the controlled application.}  Our formal analysis of
\SYSTEM{} demonstrates convergence despite noisy inputs shows how to
integrate learned variance into control theoretic guarantees.  We
demonstrate the practical benefits of these contributions by
implementing \SYSTEM{} for mobile/embedded processors to show it
provides more reliable performance and lower energy than individual,
state-of-the-art learning and control solutions.


