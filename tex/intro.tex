\section{Introduction}
% Dennard Scaling making energy essential.  Architects address energy
% by making more complicated processors which expose resources to
% software management.  For a wide range of applications, need to meet
% performance goals with minimal energy.
Large classes of computing systems---from embedded to cloud---must
deliver reliable performance to users while minimizing energy to
increase battery life or decrease operating costs.  To address these
conflicting requirements, hardware architects have begun to expose
increasingly diverse, heterogeneous resources with an array of
different performance and energy tradeoffs.  It is then software's
responsibility to allocate these resources such that performance
requirements are met with minimal energy.


% Difficulties of meeting performance with minimal energy. (1)
% complexity---heterogeneous resources---and (2) dynamics---adjust to
% unforeseen changes in workload and environment.
There are two primary difficulties in determining how to allocate
heterogeneous resources.  The first is \emph{complexity}---these
resources interact in complicated ways, leading to non-convex
optimization spaces.  The second is \emph{dynamics}---perfor\-mance
requirements must be met despite unpredictable disturbances; \eg{}
phases in input or changes in operating environment.  Prior work
addresses each of these difficulties individually.

% Prior approaches addressed each of these difficulties individually.
% ML---can handle complexity.  ML advantages: can handle
% non-convexity, avoid local optima, get to true optimal solution. ML
% disadvantages: advanced techniques are expensive and no notion of
% dynamics.  Control---handles dynamics.  Control advantages: formally
% analyzable guarantees despite dynamics.  Control disadvantages:
% relies on good models---no local optima, bounded error.
Many machine learning approaches accurately model the complex
performance/power tradeoff spaces inherent to heterogeneous computing
systems
\cite{reddiHPCA2013,dubach2010,Bitirgen2008,Ipek,Koala,LEO,Flicker,Ponamarev}.
Such ML approaches handle non-convexity, identifying local optima to
find globally optimal solutions. These techniques are computationally
expensive and lack support for dynamics; \ie{} when the environment
changes the expensive model building process must be restarted.
Control theoretic solutions provide formal guarantees that they will
converge to the desired performance despite system dynamics
\cite{Hellerstein2004a,Chen2011,PTRADE,POET,ControlWare,Agilos,grace2}.
Control provides formal guarantees that the system will converge to
the desired performance, but these guarantees require accurate models
and will fail to converge if the models do not capture all system
complexity---including local optima and non-linearities.


% Want to combine learning and control to address both difficulties
% simultaneously.  Need an interface that allows learned models to be
% used by control system.  Challenges: (1) overhead and (2) formal
% guarantees.  
Our goal is to combine learning and control to ensure performance
requirements are met with minimal energy in complex and dynamic
environments.  To meet this goal, we present \SYSTEM{},
\footnote{\textbf{C}ontrol \textbf{A}nd \textbf{L}earning for
  \textbf{O}ptimal \textbf{R}esource \textbf{E}nergy
  \textbf{E}fficiency} a \emph{parameter-free} framework for combining
machine learning and control theory to build resource allocators that
meet application performance requirements with minimal energy.
\SYSTEM{} is parameter-free because it automatically tunes all
internal parameters to customize control to the application under
management.  In constrast, many solutions require user-specified
parameters, like learning rate \cite{dubach2010} or poles of
characteristic equations \cite{ControlWare}.


\SYSTEM{}'s approach is based on a general abstraction of a control
system that factors out the parameters that must be learned.  While
traditional control designs assume all combinations of resource
configurations have been measured accurately (requiring 100s or 1000s
of samples \cite{}), \SYSTEM{} assumes that its parameters will be
tuned by a noisy learning system that estimates key parameters without
sampling them directly.  The control system's \emph{pole} is a key
parameter for guaranteeing convergence to the goal.  \SYSTEM{}
automatically tunes this parameter to guarantee the controller
converges to the desired performance---even when key parameters are
estimated rather than directly measured.  Additionally, \SYSTEM{}
implements its abstractions and self-tuning mechanisms such that the
expensive learning can be offloaded to another system and the
controller itself runs in constant---$O(1)$---time.  This separation
allows the cost of learning to be amortized over multiple devices and
allows \SYSTEM{} to use \emph{transfer learning} techniques that work
across devices and applications \cite{transferlearning}; \ie{} those
that can learn similarities between different applications and
systems.



\PUNT{
The two key
components of this interface are (1) a data structure (called the
performance hash table) that allows the controller to access the
learned model in constant ($O(1)$) time and (2) a confidence interval
and estimated standard deviation that provide probabilistic guarantees
that the combined learning and control system will converge to the
desired performance.  The \SYSTEM{} interface not only combines
control and learning techniques, but allows the learning and control
software to run on physically separate devices.  This physical
separation further mitigates the cost of expensive learning techniques
by running them on a remote server while the constant time control
systems run on the device to be managed.  In addition to amortizing
the cost of learning, moving it to a remote server allows us to take
advantage of learning techniques that work across devices and
applications; \ie{} those that can learn similarities between
different applications and systems.
}

In fact, the \SYSTEM{} is general enough to allow a wide range of
learning techniques to be paired with control systems.  So, this
approach not only provides an advantage over existing individual
learning and control techniques, it allows us to explore different
combinations of learning and control to find the best combination.

% Implement CALOREE.  Test against state of the art learning and
% self-tuning control systems.  We find that:
To evaluate \SYSTEM{}, we implement it with the learning mechanisms
running on an x86 server and the control systems working to manage
heterogeneous ARM big.LITTLE devices.  We compare \SYSTEM{} to
existing, state-of-the-art learning (including polynomial regression
\cite{}, the Netflix algorithm \cite{}, and a hierarchical Bayesian
model \cite{}) and control (including proportional-integral-derivative
\cite{} and adaptive, or self-tuning \cite{}) techniques.
Additionally, we compare to a naive combination of learning and
control that does not account for the confidence interval and standard
deviation of the learned model.  We set performance goals (in terms of
latency requirements) for a set of benchmark applications and then
measure the percentage of time the requirements are violated as well
as the energy for each application.  We test both \emph{single-app}
environments, where on application runs alone, and \emph{multi-app}
environments where other applications unpredictably enter the system
and compete for resources.  We find that \SYSTEM{} achieves the:
\begin{itemize} 
\item \textit{Most reliable performance:} 
 \begin{itemize} 
 \item In the \emph{single-app} case, the best prior learning and
   control techniques miss about 12\% of deadlines on average, the
   naive combination of learning and control misses 45\% of deadlines
   on average, but \SYSTEM{} misses only 5\% on average, reducing
   deadline misses by 50\% compared to prior approaches.
 \item In the \emph{multi-app} case, the best prior approach averages
   30\% deadline misses, the naive combination of learning and control
   averages 31\%, but \SYSTEM{} misses just 5.6\% of deadlines, a huge
   reduction compared to prior approaches.
\end{itemize}
  \item \textit{Best energy savings:} We compare to an \emph{oracle}
    with a perfect model of the application, system, and future
    events.
    \begin{itemize}
    \item In the \emph{single-app} case, the best prior approach
      averages 12\% more energy consumption than the oracle, the naive
      combination of control and learning consumes 11\% more, and
      \SYSTEM{} consumes 5\% more.  
    \item In the \emph{multi-app} case, the best prior approach
      averages 18\% more energy than the oracle, the naive combination
      of control and learning consumes 11\% more, and \SYSTEM{}
      consumes 7\% more.
    \end{itemize}
\end{itemize}

% Key contributions.
% Contributions, but I decided against bulleted llist for thsi paper
In summary, control theoretic approaches are well suited to manage
resources in dynamic environments and machine learning techniques
accurately model of complex processors.  \emph{To the best of our
  knowledge, \SYSTEM{} is the first work to combine the two at
  runtime, ensuring application performance goals without prior
  knowledge of the controlled application.}  Our formal analysis of
\SYSTEM{}'s convergence guarantees with noisy inputs shows how to
incorporate learned variance into control theoretic guarantees.  We
demonstrate the practical benefits of these contributions by
implementing \SYTSEM{} for mobile/embedded processors to show it
outperforms individual, state-of-the-art control or learning
solutions.


