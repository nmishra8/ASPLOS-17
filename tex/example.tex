\section{Background and Motivation}
\label{sec:example}

We first demonstrate how learning overcomes the challenge of
complexity, while even advanced learning systems cannot.  We then show
how control theory handles system dynamics.  We conclude by showing
how a naive combination of learning and control can lead to worse
behavior than either approach.

\subsection{\emph{Learning} Complexity}
\PUNT{
\begin{figure*}
\centering
  \subfloat[]
  {
    \includegraphics[width=.25\textwidth]{figures/STREAM-contour.png}
    \label{fig:STREAM_contour}
  }
  \subfloat[]
  {
    \input{img/STREAM-example-resized.tex}
    \label{fig:STREAM_timeline}
  }
  \subfloat[]
  {
    \includegraphics[width=.25\textwidth]{figures/BODYTRACK-contour.png}
    \label{fig:BODYTRACK_contour}
  }
  \subfloat[]
  {
    \input{img/BODYTRACK-example-resized.tex}
    \label{fig:BODYTRACK_timeline}    
  }
  \caption{\small \bf (a) Performance for \texttt{STREAM} as a
    function of configuration.  (b) Managing \texttt{STREAM}'s
    performance: \emph{Learning} navigates the complicated
    configuration space, but \emph{control}'s simple model leads to
    oscillation.  (c) Performance for \texttt{bodytrack} as a function
    of configuration. (d) Managing \texttt{bodytrack}'s performance
    when another application starts: \emph{Adaptive Control} detects
    the change and adjusts, but \emph{Learning} has no mechanism to
    handle these dynamics. }
  \label{fig:learning-models1}
\end{figure*}
}

\begin{figure}
\centering
  \subfloat[]
  {
    \includegraphics[width=.25\textwidth]{figures/STREAM-contour.png}
    \label{fig:STREAM_contour}
  }
  \subfloat[]
  {
    \input{img/STREAM-example-resized.tex}
    \label{fig:STREAM_timeline}
  }
  \caption{\small \bf (a) Performance for \texttt{STREAM} as a
    function of configuration.  (b) Managing \texttt{STREAM}'s
    performance: \emph{Learning} navigates the complicated
    configuration space, but \emph{control} leads to oscillation.}
  \label{fig:learning-models1}
\end{figure}

Many machine learning approaches estimate the most energy efficient
set of resources to allocate to an application.  These include
\emph{offline} techniques that build models using a training set and
then apply those models to new applications
\cite{Yi2003,LeeBrooks2006,CPR,ChenJohn2011,reddiHPCA2013,Paragon}.
Other approaches use \emph{online} techniques that construct models
dynamically as an application runs
\cite{Li2006,Flicker,ParallelismDial,Ponamarev,LeeBrooks}.  Finally,
\emph{hybrid} techniques combine offline modeling with online model
updates \cite{Zhang2012,packandcap,Winter2010,dubach2010,Koala,Cinder,
  wu2012inferred}.  Machine learning is well suited to building models
of complicated systems like those shown in
\figsref{fig:STREAM_contour}{fig:BODYTRACK_contour}.

To demonstrate how well learning manages complexity, we consider
meeting a performance requirement for \texttt{STREAM} on an ARM
big.LITTLE processor, which has the complicated configuration space
illustrates in \figref{fig:STREAM_contour}.  This simple memory-bound
application has complicated behavior on our test system.  Because of
the application's memory pressure, the LITTLE cores' memory hierarchy
cannot deliver the required performance.  The big cores' more powerful
memory hierarchy delivers much greater performance, but the peak
performance occurs with 3 big cores.  Furthermore, at low clockspeeds,
these three big cores cannot saturate the memory bandwidth, while at
high clockspeeds the performance actually drops as the processor
overheats and triggers thermal management.  For \texttt{STREAM}, then,
the peak speed occurs using 3 big cores at 1.2 GHz, and it is not
energy efficient to spend any time on the LITTLE cores.  This
application, however, does not have distinct phases, so once a
resource allocator finds the most energy efficient configuration, it
simply needs to maintain it.

We launch the application and use both a hierarchical Bayesian
\emph{learning} model \cite{LEO} and an \emph{adaptive control} system
\cite{POET} to meet a performance requirement with minimal energy.
The \emph{learning} approach estimates the application's performance
and power for all configurations and then uses the lowest power
configuration that delivers the required performance.  The
\emph{adaptive control} approach begins with a generic model of
power/performance tradeoffs.  As the controller runs, it continually
measures performance and adjusts both allocated resources and its
internal models to tailor its response to the current application.
While many controllers use linear models, this adaptive controller
dynamically adjusts to non-linearities with a series of linear
approximations; however, the adaptive controller is sensitive to local
maxima, which can cause it to oscillate.

\figref{STREAM_timeline} shows the results of controlling 20
iterations of \texttt{STEAM} to meet the performance requirement.  The
x-axis shows iteration number and the y-axis shows performance
normalized to the requirement.  The learning approach achieves the
goal, but the adaptive controller oscillates wildly around it,
sometimes not achieving the goal and sometimes delivering performance
that is too high (and wastes energy). The oscillations occur because
the controller's adaptive mechanisms cannot handle the non-convexity
in STREAM's behavior, a known limitation of adaptive control systems
\cite{LEO,ICSE2014}.  Hence, the \emph{learner}'s ability to handle
complex behavior is crucial for reliable performance in this example.

% This result may be somewhat counter-intuitive.  The problem is that
% the controller cannot handle \texttt{STREAM}'s complexity.  One way to
% address this problem would be to build a custom controller just for
% this application, but that controller would not be useful for other
% applications.  In contrast, the learner can find the local maxima in
% the configuration space, and as this application has no phase changes
% or other dynamics, the one configuration that the learner finds is
% suitable for the entire application.

\subsection{\emph{Controlling} Dynamics}

\PUNT{
\begin{figure*}
\centering
  \subfloat[]
  {
    \includegraphics[width=.25\textwidth]{figs/kmeans.png}
    \label{fig:kmeans_contour}
  }
  \subfloat[]
  {
    \input{img/kmeans-example-resized.tex}
    \label{fig:kmeans_timeline}    
  }
  \caption{\small \bf (a) Performance for \texttt{kmeans} as a function of
    configuration.  (b) Managing \texttt{kmeans}' performance when
    another application starts: \emph{Control} detects the change and
    adapts, but \emph{learning} has no mechanism to handle these
    dynamics.}
  \label{fig:learning-models2}
\end{figure*}
} 


\begin{figure}
\centering
  \subfloat[]
  {
    \includegraphics[width=.25\textwidth]{figures/BODYTRACK-contour.png}
    \label{fig:BODYTRACK_contour}
  }
  \subfloat[]
  {
    \input{img/BODYTRACK-example-resized.tex}
    \label{fig:BODYTRACK_timeline}    
  }
  \caption{\small \bf (a) Performance for \texttt{bodytrack} as a
    function of configuration. (b) Managing \texttt{bodytrack}'s
    performance when another application starts: \emph{Adaptive
      Control} detects the change and adjusts, but \emph{Learning} has
    no mechanism to handle these dynamics. }
  \label{fig:control}
\end{figure}


We now consider a dynamic environment.  We begin with
\texttt{bodytrack} as the only application running on the system.
Halfway through its execution, we launch a second app
(\texttt{STREAM}) on a single big core, dynamically altering resource
availability. \texttt{bodytrack}'s behavior is simpler than
\texttt{STREAM} as shown in \figref{fig:BODYTRACK_contour}; it
achieves its best performance on 4 big cores at the maximum
clockspeed.  The toughest part of allocating resources for this
application is determining how much work to do on the LITTLE cores (to
conserve energy) while still meeting the performance requirements.
The relationship between LITTLE and big core performance is
non-linear.

\figref{bodytrack_timeline} shows the results of this experiment.  The
vertical dashed line---at frame 99---represents when the second
application begins.  The figure clearly shows the benefits of the
adaptive control system in this dynamic scenario.  When the second
application starts, the controller detects the change in \texttt{bodytrack}'s
performance and then changes resource allocation (increasing
clockspeed and moving bodytrack from 4 to 3 big cores).  The
controller is not aware of the second application, rather it observes
\texttt{bodytrack}'s performance drop at frame 99 and immediately
restores performance in the next frame. The learning system however,
does not have any inherent mechanism to measure the change or adapt to
the altered performance.  While we could theoretically add feedback to
the learner and re-estimate the configuration space whenever the
environment changes, doing so is clearly impractical.

Control systems are a light-weight mechanism for managing such
dynamics \cite{Hellerstein2004a}. Adaptive control systems are
resilient to many external effects that alter performance.  For this
reason, control systems have proven especially useful in webservers
with fluctuating request rates
\cite{Horvarth,LuEtAl-2006a,SunDaiPan-2008a} and multimedia
applications with dynamically varying inputs
\cite{TCST,Agilos,grace2}.  These prior control solutions, however,
are always highly application dependent---with application-specific
models encoded in the controller's design---making a controller for
video playback unsuitable for controlling GPS navigation.  The main
drawback of even adaptive control is that it can fail given unmodeled
non-convexity in the relationship between resources and performance.
This limitation is why existing adaptive control approaches are built
for specific classes of applications---specializing for the class
accounts for common non-convexities of that class.

\subsection{Combining Control and Learning}
The above examples highlight the complementary advantages of both
learning and control.  Since sophisticated learning systems can
identify non-convexities and control system can adapt to dynamic
changes, it seems natural to combine the two.  All control systems are
designed based on models of resource behavior---a process called
\emph{system identification} \cite{ControlHandbook}.  It therefore
seems that a straightforward solution would simply use the learned
models to devise a controller on a per application basis.
%The only complication is turning the typically absolute models
%produced by the learners into \emph{difference} models required by
%control systems.\footnote{In a continuous system we would use
%  differential models, as all resources in our system are discrete, we
%  only use difference models in this paper.} 
There are two complications to executing this combination: (1) the
overhead of the learning mechanism and (2) the probabilistic nature of
the learned models.  

The sophisticated learning methods---including polynomial multivariate
regression \cite{}, the Netflix algorithm \cite{}, and hierarchical
Bayesian models \cite{}---that can handle complicated configuration
spaces like those shown above are computationally very expensive.
\tblref{} shows the time required to produce a model for each of three
representative learning approaches.  We can see from these timings
that learning overhead is a serious consideration, especially for
energy-limited mobile and embedded systems. \TODO{Need to add learning
  overhead table.}

The second complication is that controller design assumes the models
on which they are built represent ground truth.  The typical model
building approach used in control design is far more expensive than
the above learning methods---rather than sample the space and estimate
unobserved resource configurations, control system identification
takes multiple measurements of each configuration
\cite{ICSE2014,ControlHandbook}.  The difference between the learned
models and the typical models used for control design, is that the
learned models have a much higher possible margin of error.


\begin{figure}
\centering
\input{img/BODYTRACK-example2-resized.tex}
\caption{\small \bf Comparison of existing adaptive control and a
  naive combination of control and learning.}
\label{fig:not-simple}
\end{figure}

To demonstrate this difficulty, we use the model produced by the
hierarchical Bayesian learner to synthesize an adaptive controller for
\texttt{bodytrack} assuming the learner's model is error free.  We
rerun the previous experiment; the results are shown in
\figref{fig:not-simple}.  This combination of learning and control
actually produces worse results than either learning or control by
themselves.  As shown in the figure, before the second application
starts, the combined approach oscillates around the target.  When the
second application begins, the oscillations continue.  In this case,
the learning method failed to identify a local optima, but the
controller takes the model as ground truth. Although it is an adaptive
control system, it cannot distinguish between application noise and
actual error and cannot cancel the oscillations.

The next section describes our approach to combining learning and
control that amortizes the learner's overhead and incorporates the
learner's confidence interval and estimated variance into the control
design. Incorporating this information allows us to combine multiple
different learning approaches with control systems while still
providing formal guarantees that the control system will converge to
the desired performance with a user-specified confidence interval.
