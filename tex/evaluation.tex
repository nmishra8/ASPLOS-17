\section{Experimental Setup}
\begin{figure}[t]
  \subfloat[]
  {
    \includegraphics[width=.22\textwidth]{figures/odroid.png}
    \label{fig:odroid}
  }
  \subfloat[]
  {
    \includegraphics[width=.22\textwidth]{figures/odroidall.png}
    \label{fig:odroid_all}
  }
 \caption{ODROID-XU3 boards used in the evaluation.}
 \label{fig:odroidall}
\end{figure}

\subsection{Platform and Benchmarks}
We run streaming applications on four ODROID-XU3 devices running
Ubuntu 14.04, as shown in \figref{fig:odroidall}. The ODROIDs have
Samsung Exynos 5 Octa processors using the ARM big.LITTLE
architecture.  Each has 19 speed settings for the 4 big cores and 13
for 4 LITTLE cores.  Each board has an on-board power meter updated at
1/4 \ms intervals, this meter captures core, GPU, memory, and flash
drive power.  Each resource configuration (combination of big/LITTLE
cores and clock-speeds) has a different performance and power, which
is application-dependent.

We use 20 benchmarks from different suites including PARSEC
\cite{parsec}, Minebench \cite{minebench}, Rodinia \cite{rodinia}, and
STREAM \cite{stream}. \figref{fig:application_variety} shows the
variety of workloads indicated by the \emph{lack-of-fit} or the
absence of correlation between frequency and performance.
Applications with high lack-of-fit do not speed up with increasing
frequency---typical of memory bound applications. Applications with
low lack-of-fit do see increasing performance with increasing
clock-speed \cite{powerslope}.  Applications with intermediate
lack-of-fit tend to improve with increasing clock speed up to a point
and then see no further improvement.  Each application has an outer
loop which processes one unit in a data stream (\eg{} a point for
\texttt{kmeans} or a frame for \texttt{x264}). The application signals
the completion of processing a single stream element using a standard
API \cite{icac2010heartbeats}.  Performance targets are specified as
application-specific latencies for these stream elements.



\begin{figure}[t]
  \input{img/mem-compute.tex}
   \vskip -1em
   \caption{\emph{Lack-of-fit} for performance vs clock-speed for a
     fixed number of cores. Lower lack-of-fit indicates a more
     compute-bound application, higher values indicate a memory-bound
     one.}
  \label{fig:application_variety}
\end{figure}


\subsection{Evaluation Metrics}
%\SYSTEM{} uses the model for power and performance and its controller
%actively uses this knowledge to meet the performance target. 
The latency targets represent performance requirements. To test a
variety of requirementswWe run our applications with targets that
represet 50-90\% of the maximum speed and evaluate \SYSTEM{} under
each constraint. We quantify the performance reliability by measuring
the number of deadlines that were missed for each application and
performance target. If the application processes $n$ elements total
and $m$ of those elements took longer than the target latency we
compute deadline misses as:
\begin{equation}
deadline\, misses = 100\% \cdot \frac{m}{n}.
\end{equation}

We evaluate energy savings by constructing an oracle.  We run every
application in every resource configuration and record performance and
power for every stream element.  By post-processing this data we
determine the optimal resource configuration for each stream element
and performance target. To compare across applications, we normalize
energy:
\begin{equation}
  normalized\,energy = 100\% . \left( \frac{e_{measured}}{e_{optimal}} - 1 \right)
\end{equation}
where $e_{measured}$ is measured energy and $e_{optimal}$ is the
optimal energy produced by our oracle. We subtract 1, so that this
metric shows the percentage of energy over optimal.  

\subsection{Points of Comparison}
We compare \SYSTEM{} to a number of existing learning and control
approaches:
\begin{enumerate}
\item \textit{Race-to-idle}: This well known heuristic allocates all
  resources to the application to complete each stream element as fast
  as possible, then idles until the next element is available
  \cite{kim-cpsna,powerslope,heisner}.  This heuristic requires no
  knowledge of the application and never misses deadlines in a
  single-application scenario.
\item \textit{PID-Control}: This is a standard single-input
  (performance), multiple-output (big/LITTLE core counts and speeds)
  proportional-integral-controller representative of several that have
  been proposed for computer resource management
  \cite{Hellerstein2004a,METE}.  This controller is tuned to provide
  the best average case behavior across all applications and targets.
\item \textit{Online}: observes the new application in small number of
  configurations then performs polynomial multivariate regression to
  estimate unobserved configurations' performance and power. Several
  approaches have used similar regression based models for resource
  allocation \cite{Koala,Lee2006,others}.
\item \textit{Offline}: does not observe any values for the current
  application---instead using previously observed applications to
  estimate power and performance as a linear regression.  This
  baseline represents the class of offline learners that build models
  with a training set and apply them without observing the new
  application \cite{PUPiL,LeeBrooks2006,CPR}.
\item \textit{Nuclear}: is a machine learning approach popularized as
  the ``Netflix'' algorithm. Variations of this approach have been
  used to allocate heterogeneous resources in data centers
  \cite{Paragon,quasar}.
\item \textit{HBM}: is a hierarchical Bayesian learner previously used
  to allocate resources to meet performance goals with minimal energy
  in server systems \cite{LEO}.
\item \textit{Adaptive-Control}: a state-of-the-art, adaptive control system
  designed to meet application performance with minimal energy
  \cite{POET}.  POET requires users to specify a model.  We use POET
  with the model produced by the \emph{Offline} learner. A recent
  study comparing ML and control techniques for resource allocation
  showed that there was no single best approach, but adaptive control
  had the best average behavior across a number of applications
  \cite{TAAS}.
\end{enumerate}


\SYSTEM{} is a framework that allows different learners to be combined
with the \SYSTEM{} control system.  We therefore the above baselines
to four different versions of \SYSTEM{}:
\begin{enumerate}
\item \textit{CALOREE-NoPole}: uses the HBM to produce a model, but
  sets the pole set to 0.  We include this baseline to show the
  importance of informing the controller of the confidence interval
  and model variance. In all other versions of \SYSTEM{} we set the
  pole according to \secref{guarantees}.
\item \textit{CALOREE-online}: uses the online learner.
\item \textit{CALOREE-nuclear}: uses the Nuclear learner.
\item \textit{CALOREE-HBM}: uses the HBM learner.
\end{enumerate}

In all cases that require prior knowledge, we ensure that knowledge of
the application under test is never included in that set of prior
knowledge.  Specifically, we use leave-one-out cross validation: to
test application $x$, we form a set of all other applications, train
the models, and then test on $x$.

\section{Experimental Evaluation}

\subsection{Performance and Energy for Single App}
%Setup
\begin{figure}[!h]
\centering
  \input{img/single-app-summary2-v4.tex}
   \vskip -.5em
  \caption{Summary data for single-app scenario.}
  \label{fig:single-sum}
\end{figure}


We set a range of performance targets from 50-90\% of the maximum
achievable performance and measure the MAPE (performance error) and
energy efficiency (energy over optimal) for all points of comparison.
\figref{fig:single-sum} represents the summary results as an average
error across all targets for single application scenario. This figure
shows two charts with the percentage of deadline misses in the top
chart and the energy over optimal in the bottom.  The dots show the
average for each technique, while the error bars show the minimum and
maximum values.  

Not surprisingly, race-to-idle meets all deadlines, but its
conservative resource allocation has the highest average energy
consumption. Among the prior learning approaches Nuclear has the
lowest average deadline misses (11\%), but with high energy (40\% more
than optimal), while the HBM has higher deadline misses (17\%) but
with significantly lower energy consumption (16\%). Adaptive control
achieves similar deadline misses (14\%) with lower average energy than
any of the prior learning approaches (12\%). \SYSTEM{} with no pole
misses 45\% of all deadlines, which is clearly unacceptable.

When we allow \SYSTEM{} to adaptively tune its pole, however, we see
greatly improved results.  The best combination is \SYSTEM{} with the
HBM, which misses only 5.5\% of deadlines on average, while consuming
just 4.4\% more energy than optimal.  These numbers represent large
improvements in both performance reliability and energy efficiency
compared to prior approaches.  The other learners paired with
\SYSTEM{} achieve similar results to the prior adaptive control
approach.

The summary data shows us that the best prior approaches are
race-to-idle, HBM, and adaptive control.
\figsref{fig:single-perf}{fig:single-energy} show the detailed results
for the 50, 70, and 90\% targets comparing the best of these prior
approaches to \SYSTEM{} with no pole and \SYSTEM{} coupled with the
HBM, others data has been ommitted for space.  The benchmarks are
shown on the x-axis; the y-axis shows the number of deadline misses
and the normalized energy, respectively.




\begin{figure*}[!t]
  \input{img/single-app-performance-small.tex}
   \vskip -.5em
  \caption{Comparison of application performance error for single application scenario.}
  \label{fig:single-perf}
\end{figure*}


\begin{figure*}[!t]
  \input{img/single-app-energy-small.tex}
   \vskip -.5em
  \caption{Comparison of application energy consumption for single application scenario.}
  \label{fig:single-energy}
\end{figure*}

%In addition to providing better average case behavior, \SYSTEM{}
%provides significantly better worst case behavior.  


\subsection{Performance and Energy for Multiple Apps}
\begin{figure}[!h]
\centering
  \input{img/multi-app-summary2-v4.tex}
   \vskip -.5em
  \caption{Summary data for multi-app scenario.}
  \label{fig:multi-sum}
\end{figure}

We again launch each benchmark with a performance target (using the
same targets as the prior study).  One third through execution, we
start another application randomly drawn from our benchmark set, which
we bind to one big core.  Launching this second application changes
performance and power consumption.  Delivering performance to the
original application in this dynamic scenario tests the ability to
react to environmental changes.

\figref{fig:multi-sum} summarizes the results as an average error
across all targets for the multiple application scenario.  This figure
again summarizes latency and energy results for all baselines.  We
note that some targets are unachievable for some applications.  In
this case, race-to-idle cannot meet all deadlines, partly because some
are unachievable, but more importantly because it cannot make use of
LITTLE cores to do some work.  In fact, most approaches do badly in
this scenario.  Even adaptive control misses 50\% of the deadlines in
this scenario.  \SYSTEM{} with the HBM produces the lowest deadline
misses with an average of 20\%.  It also produces the second lowest
energy, using slighly more than race-to-idle because it uses LITTLE
cores to make up for some work that cannot be done on a big core due
to the second application.  \figsref{fig:multi-perf}{fig:multi-energy}
shows the detailed results.  The 90\% target is generally not
reachable in this scenario as it would require the controlled
application to have exclusive use of all big cores.


\PUNT{
Across all applications and targets, the online model produces an
average error of 21\%, the offline: 24\%, HBM: 19\%, POET: 7.4\%,
CALOREE-NoPole: 11\%, and \SYSTEM{}: 2.8\%. \SYSTEM{} is also more energy
efficient than the baseline algorithms. Across all applications and
targets, the average normalized energy for online model is 22\%, the
offline: 21\%, HBM: 17\%, POET: 34\%, CALOREE-NoPole: 13\%, and \SYSTEM{}:
8.5\%.
}
POET and \SYSTEM{} provide more reliable performance because they are
specifically designed to handle system dynamics.  \SYSTEM{}, however,
does significantly better than POET in the worst case because the HBM
produces more robust models.  Again, the results show the benefits of
incorporating model variance into the controller's pole calculation.


\begin{figure*}[htp!]
  \input{img/multi-app-performance-small.tex}
   \vskip -.5em
  \caption{Comparison of application performance error for multiple application scenario.}
  \label{fig:multi-perf}
\end{figure*}

\begin{figure*}[!t]
  \input{img/multi-app-energy-small.tex}
   \vskip -.5em
  \caption{Comparison of application energy consumption for multiple application scenario.}
  \label{fig:multi-energy}
\end{figure*}

\subsection{Multiapp Details for \texttt{bodytrack}}
To demonstrate \SYSTEM{} in this dynamic environment we look at the
specific example of \texttt{bodytrack} with a 70\% target, shown in
\figref{fig:bodytrack-multiapp} with time on the x-axis and
performance/power on the y-axis.  Performance is normalized to the
target.  There is a curve for each of HBM, POET, and \SYSTEM{}. The
vertical dashed line shows the point where the second application
starts.

\PUNT{This example illustrates \SYSTEM{}'s benefits over prior
  approaches that use only control or learning.  There are two key
  regions in the figures, the times before the second application
  starts (on the left of the vertical dashed line) and the times after
  (on the right).}  

Before the second application starts (at the dashed line), both HBM
and \SYSTEM{} do a good job of tracking the target and keeping energy
low.  In contrast, POET produces oscillating performance (and thus
power) because it has a bad model of \texttt{bodytrack}'s LITTLE core
performance.  This oscillation also results in unnecessary power
consumption.  After the second application starts, POET and \SYSTEM{}
recognize the performance has changed and adjust resource usage.
\SYSTEM{} does a slightly better job tracking the change, producing
fewer oscillations.  HBM cannot adjust to the change as it computes
the optimal configuration once at the beginning of the application.
Overall, HBM produces a MAPE of 13\%, POET's is 9\%, and \SYSTEM{}'s
is 4\%.  \SYSTEM{} produces better results than HBM because it reacts
to the change; it produces better results than POET because it has
captured the complex application-specific behavior on this system.




\begin{figure}[t]
  \input{img/bodytrack-multiapp.tex}
   \vskip -.5em
  \caption{Time series of \texttt{Bodytrack} with phase change caused by multiple applications running.}
  \label{fig:bodytrack-multiapp}
\end{figure}


\PUNT{
\subsection{Power and performance estimation}
We use HBM as described in \secref{sec:framework:HBM} to estimate the
power and performance for the applications using only 10 out of 128
observations.

Our results are summarized in \figref{fig:accuracy}. HBM is 13.4\%
better than online and 19.1\% better than offline in terms of
performance estimation and HBM is 3.7\% better than online and 1.5\%
better than offline in terms of power estimation, on average over all
the benchmarks.
%Infact, LEO uniformly performs well for all applications with the worst accuracy being YY as compared to offline and online algorithm.


\begin{figure}[t]
  \input[width=\columnwidth]{img/accuracy-performance.tex}
   \vskip -.5em
  \caption{Accuracy performance}
  \label{fig:accuracy}
\end{figure}
}

%\PUNT{
\subsection{Phase Change}
\begin{figure}[!h]
  \input{img/x264-phases.tex}
   \vskip -.5em
  \caption{Time series of \texttt{x264} with phase change caused by different inputs.}
  \label{fig:x264-phase-change}
\end{figure}
In this experiment we demonstrate that \SYSTEM{} allows applications
to operate well by changing resource allotment when the input varies
with time. In \figref{fig:x264-phase-change} we see \texttt{x264}, a
video encoder application with 2 different phases, where the phase
change occurs at the $500^{th}$ frame. The first scene is difficult
and the second one becomes significantly easier. In the first phase,
\SYSTEM{} and the baseline HBM seem to have lower MAPE as compared to
POET.  At the same time, these two algorithms seem to consume less
energy than POET indicating that POET is operating at a configuration
not on the Pareto frontier of power and performance.  When we change
phase, \SYSTEM{} and HBM both algorithms are still able to meet the
performance target with far less fluctuations as compared to POET.
But, \SYSTEM{} provides better energy savings as compared to HBM.


%}


\subsection{The Pole's Importance}

\figref{fig:lavamd_contour} (see \secref{example}) shows that LAVAMD has a
complicated tradeoff space on our test platforms.  \secref{guarantees}
presents an analytical argument that tuning the controller to model
variance and confidence interval will prevent oscillation and provide
probabilistic control theoretic guarantees.  We now demonstrate this
empirically by showing LAVAMD's single-app behavior controlled by both
CALOREE-NoPole and \SYSTEM{} to meet the 80\% target.

\figref{fig:lavamd-pole} shows the results, with time on the x-axis
and normalized performance and power on the respective y-axis.  As is
clear from the figure, CALOREE-NoPole oscillates around the desired
performance and causes wide fluctuations in power consumption.  In
contrast, after a brief period of oscillation to adjust to the new
model, \SYSTEM{} provides reliable performance right at the target
value (normalized to 1 in this case).  \SYSTEM{} also saves tremendous
energy because it does not oscillate but uses a constant mixture of
big and LITTLE cores to keep energy near minimal.  
% \TODO{Do we need the vertical line here?  This is a single app run
%   right?}

\begin{figure}[t]
  \input{img/LAVA-pole.tex}
   \vskip -.5em
   \caption{The pole's effects on LAVAMD behavior.}
  \label{fig:lavamd-pole}
\end{figure}


\subsection{Sensitivity to the Measured Samples}
We examine \SYSTEM{}'s sensitivity to sample size.  We vary the number
of samples taken online and show how it affects model accuracy.  This
is simply the HBM's accuracy in producing the model used by the
controller.  This number is significant, because the smaller the
number of samples, the faster the controller can switch from a general
model to the HBM's application specific model.

\figref{fig:sensitivity} compares \SYSTEM{}'s accuracy to Online for
learning both performance (top) and power (bottom).  The figure shows
sample size on the x-axis and accuracy on the y-axis.  \SYSTEM{}'s HBM
initially performs as well as the Offline approach and as sample size
increases, the accuracy uniformly improves and reaches greater than
90\% around 20 samples. The online approach needs at least 7 samples
to begin model building.  As we get more samples the accuracy for the
online model improves but still does not meet \SYSTEM{}'s accuracy for
the same number of samples.

\begin{figure}[t]
  \input{img/sample-accuracy-v3.tex}
   \vskip -.5em
  \caption{Estimation accuracy versus sample size.}
  \label{fig:sensitivity}
\end{figure}

%\PUNT{

\subsection{Overhead}
\SYSTEM{}'s main source of overhead is sampling where the applications
need to run through a few configurations before \SYSTEM{} can reliably
estimate the entire power and performance frontier. We argue that the
sampling cost can be distributed across devices by asking each of them
to contribute samples for estimation. Once the sampling phase is over,
the HBM is quite fast and can generate an estimate as fast as 500 ms
which is significantly smaller than the time required for sampling the
applications.  Using four ODROIDs, each board only needs to contribute
4 samples to achieve 90\% accuracy.  In this worst case
(\texttt{facesim}), this sampling overhead is less than 2\%.  For all
other benchmarks it is lower, and for most it is negligible.

The GCS requires only a few floating point operations to execute, plus
the table lookups in the PHT.  To evaluate its overhead, we time 1000
iterations.  We find that it is under 2 microseconds, which is
significantly faster than we can change any resource allocation on our
system.  We conclude that the GCS has negligible impact on performance
and energy consumption of the controlled device.










