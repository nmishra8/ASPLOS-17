\section{Experimental Setup}

\subsection{Platform and Benchmarks}
We perform our experiments on four ODROID-XU3 devices which are a
small mobile devices with Ubuntu 14.04 running on them. The ODROID
boards have Samsung Exynos 5 Octa processors based on the ARM
big.LITTLE architecture.  Each processor has 19 speed settings for its
4 big cores and 13 for 4 LITTLE cores.  Each board has a power meter
updated at 1/4 \ms intervals.  Each resource configuration
(combination of big/LITTLE cores and clock-speeds) has a different
performance and power, which is, itself, application-dependent.

We use 20 different benchmarks from different suites including PARSEC
\PUNT{ (\texttt{blackscholes}, \texttt{facesim}, \texttt{ferret},
  \texttt{x264})} \cite{parsec}, Minebench \PUNT{(\texttt{Kmeans},
  \texttt{non fuzzy kmeans \\* (Kmeansnf)})} \cite{minebench}, Rodinia
\PUNT{
  (\texttt{backprop}, \texttt{cfd}, \texttt{nn}, \texttt{lud}, \\*
  \texttt{backprop}, \texttt{bfs})}\cite{rodinia}, and STREAM
\cite{stream}. Our benchmarks include compute-bound, memory-bound, and
hybrid workloads as shown in \figref{fig:application_variety}.  That
figure reports the \emph{lack-of-fit} or the lack of correlation
between frequency and performance.  Applications with high lack-of-fit
do not improve performance with increasing frequency, typical of
memory bound applications, while applications with low lack-of-fit do
see increasing performance with increasing clock-speed
\cite{powerslope}. Applications with intermediate lack-of-fit tend to
improve with increasing clock speed up to a point and then see no
further improvement.  Each application has been instrumented to report
its performance as heartbeats/second an application-specific
performance metric \cite{POET}.
\begin{figure}[t]
  \input{img/mem-compute.tex}
   \vskip -1em
  \caption{\emph{Lack-of-fit} for performance vs clock-speed for a fixed number of cores. Lower value for lack-of-fit indicates a more compute-bound application whereas a higher value indicates a memory bound application.}
  \label{fig:application_variety}
\end{figure}


\subsection{Evaluation Metrics}
\PUNT{
We evaluate learning methods by computing the \emph{accuracy} of the
learned models:
\begin{equation}
\label{eq:accuracy}
accuracy(\hat{\y},\y) = max\left(1 - \frac{\| \hat{\y}-\y \|^2_2 }{\| \y - \bar{\y}\|^2_2},0\right).
\end{equation}
where $\hat{\y}$ represents the predicted performance/power values
and $\y$ is the true value of the power and performance.
}

%\SYSTEM{} uses the model for power and performance and its controller
%actively uses this knowledge to meet the performance target. 
A performance target can be thought of as a quality of service
expectation for the given application. We run our applications from
10\% to 90\% performance targets and observe how \SYSTEM{} performs
under each constraint. We quantify the ability to meet performance
targets using the standard metric \emph{mean absolute percentage
  error} (MAPE).  Suppose the application wants to emit $n$ heartbeats
while maintaining a speed of $S_r$, then MAPE is calculated as:
\begin{equation}
MAPE = 100\% \cdot \frac{1}{n} \sum\limits_{i=1}^{n} max \left( \frac{S_{r} - s_m(i)}{S_r},0 \right)
\end{equation}
where $S_{r}$ is the performance target and $s_m(i)$ is the
performance observed for $i$th heartbeat.


%We evaluate energy savings by constructing an oracle.  
We run every
application in every resource configuration and record performance and
power for every heartbeat.  By post-processing this data we can
determine the optimal configuration for each heartbeat and each
performance target. To produce an energy metric that we can compare
across applications, we normalize energy as:
\begin{equation}
  normalized\,energy = 100\% . (e_m / e_{optimal} - 1)
\end{equation}
where $e_m$ is the measured energy and $e_{optimal}$ is the optimal
energy produced by our oracle. We subtract 1, so that this metric
shows the proportion of energy consumed over optimal. 
\PUNT{
 \TODO{Should we
  multiply this by 100, so that it is also a \%?  That seems like a
  good idea -- might as well be consistent across the two metrics.  We
  don't even have to change the data, just the axis labels. }
}

\subsection{Points of Comparison}
We compare \SYSTEM{}'s combination of HBM with control to baselines
constructed with a combination of different learning models and a
state-of-art controller based on the \emph{Pace-to-idle (P2I)}
heuristic \cite{kim-cpsna}.  Pace-to-idle is proven never to be worse
than race-to-idle and often provides as much as $2\times$ energy
savings, but requires application-specific knowledge to implement.
Pace-to-idle completes jobs in the most energy-efficient configuration
and then transitions to a low-power sleep state until the next job is
ready.  As the energy-efficiency of a configuration is
application-dependent, it is natural to combine pace-to-idle with
machine learning approaches that can estimate the energy efficiency
for different applications.  In the next section, we will compare
\SYSTEM{}'s delivered performance and energy savings with:
\begin{enumerate}
\item \textit{Online-P2I} -- This strategy observes the
  application at small number of configurations then performs
  polynomial multivariate regression for the performance and
  power of unobserved configurations. \PUNT{We use the \textit{Online}
    learning model in combination with \emph{Pace-to-idle (P2I)}
    heuristic.}
\item \textit{Offline-P2I} -- This is our zero-sample policy, used
  when we do not observe any values for the current application. This
  method takes the mean over all known applications and uses that to
  estimate power and performance for new applications. This strategy
  only uses prior information and does not update the model based on
  runtime observations. \PUNT{We use the \textit{Offline} learning
    model in combination with \emph{Pace-to-idle (P2I)} heuristic.}
\item \textit{LEO-P2I} -- We use the \textit{LEO} learning model in
  combination with \emph{Pace-to-idle (P2I)} heuristic.
\item \textit{POET} -- A state-of-the-art, open source control system
  designed to meet application performance with minimal energy
  \cite{POET}.  POET requires users to specify a model of resource
  performance and power consumption.  We use POET with the model
  produced by the \emph{Offline} learner.
\end{enumerate}

\section{Experimental Evaluation}
We evaluate \SYSTEM{}'s ability to deliver requested performance with
near-optimal energy.  We first examine performance accuracy and energy
in a single-application scenario, where each application is run by
itself.  We then consider a multi-application scenario where each
application is run and then, halfway through, a second random
application is launched, testing the ability to deliver performance in
a changing environment.  We then examine other dynamic scenarios where
application inputs or behavior changes during application execution.

\subsection{Performance and Energy for Single App}
%Setup
To test the ability to deliver performance and minimize energy, we set
a range of targets ranging from 10-90\% of the maximum achievable
performance on our system. The extreme targets are generally easy to
hit. The most interesting performance targets are around 30\%-70\%,
where there are not obvious choices for configuration settings. These
intermediate targets require a blend of big and LITTLE cores and
getting that blend to both meet the performance and reduce energy is
dependent on accurate models of performance and power.

% Results
The results for these single-application tests are shown in
\figsref{fig:single-perf}{fig:single-energy}.  The benchmarks are
shown on the x-axis; the y-axes show MAPE and the normalized energy,
respectively.  We find that \SYSTEM{} has lower MAPE and energy
consumption than the baseline algorithms. Across all applications and
targets, the online model produces an average error of 5.4\%, the
offline: 4.5\%, LEO: 4.0\%, POET: 4.7\%, and \SYSTEM{}: 2.0\%.  These
results show that \SYSTEM{} reduces error by a factor of two compared
to prior approaches.

The energy consumption results are even more impressive.  The online
model requires an average of 52\% more energy than optimal, offline:
25\%, LEO: 31\%, POET: 26\%, and \SYSTEM{}: 7\%.  Thus the combination
of learning and control provides a dramatic reduction in energy
consumption compared to even state-of-the-art learning or control
approaches.  The reason is that the combination is complementary: LEO
produces accurate models while control both can correct small errors
in those models and adapt to dynamic changes in behavior.

\begin{figure*}[t]
  \input{img/single-app-performance.tex}
   \vskip -1em
  \caption{Single applications MAPE}
  \label{fig:single-perf}
\end{figure*}

\begin{figure*}[t]
  \input{img/single-app-energy.tex}
   \vskip -1em
  \caption{Single applications energy}
  \label{fig:single-energy}
\end{figure*}

%In addition to providing better average case behavior, \SYSTEM{}
%provides significantly better worst case behavior.  


\subsection{Performance and Energy for Multiple Apps}
In this experiment, we test the ability to deliver performance in an
environment where applications compete for resources.  We launch each
of our benchmarks with a performance target (using the same targets as
the prior study).  Halfway through execution, we start another
application randomly drawn from our benchmark set.  We bind this
application to one big core.  Launching this second application
clearly changes performance and power consumption.  Delivering
performance to the original application in this dynamic scenario
clearly tests the ability to react to environmental changes.

The results for the multi-application tests are shown in
\figsref{fig:multi-perf}{fig:multi-energy}.  The benchmarks are shown
on the x-axis; the y-axes show MAPE and the normalized energy,
respectively.  We note that the 90\% target is generally not reachable
in this scenario as it would require the controlled application to
have exclusive use of all big cores.


\SYSTEM{} again has lower MAPE than the baseline algorithms. Across
all applications and targets, the online model produces an average
error of 38.5\%, the offline: 15.5\%, LEO: 19.7\%, POET: 21\%, and \SYSTEM{}:
18\%. \SYSTEM{} is also more energy efficient than the baseline algorithms. Across
all applications and targets, the average normalized energy for online model is 22\%, the offline: 13.5\%, LEO: 18\%, POET: 15\%, and \SYSTEM{}: 14\%. But, the main benefits of \SYSTEM{} can be seen in terms of worst case scenarios where \SYSTEM{} is significantly better than the baselines. The online approach has a highest MAPE
of 863\%, for  Offline: 307\%,  LEO: 747\%, POET: 158.7\% and \SYSTEM{}: 110\%.  \SYSTEM{} is also more energy efficient in the worst case scenario. And the worst case energy consumption for online approach is 84\%, for offline: 72\%, for LEO: 84\%, for POET: 37\% and for \SYSTEM{}: 30\%. POET and \SYSTEM{} both produce better outcomes in the
dynamic scenario because they are specifically designed to handle
system dynamics.  \SYSTEM{}, however, does significantly better in the
worst case than POET because the HBM produces more robust models.
 %In this case the average numbers skew \SYSTEM{}'s benefit as no approach does very well with the 90\% target and all but online do well at the 10\% target.  So, we also consider worst case behavior for all targets.
\PUNT{
The online approach has a highest MAPE
of 80\% for LUD at the 70\% target.  Offline's highest MAPE is 71\%
for STREAM at the 30\% target.  LEO's highest MAPE is 70\%, also for
STREAM at 30\%. POET's highest MAPE of 35\% also occurs for STREAM at
30\%.  \SYSTEM{}'s highest MAPE of 24\% occurs with LUD at the 70\%
target.  POET and \SYSTEM{} both produce better outcomes in the
dynamic scenario because they are specifically designed to handle
system dynamics.  \SYSTEM{}, however, does significantly better in the
worst case than POET because the HBM produces more robust models.
}
The energy consumption results are not as meaningful for this
experiment, but they are included for completeness.  Because there is
another application running that is not under control, it consumes
resources and energy that drags down energy efficiency for the control
approaches.  The learning approaches are not affected the same way.
Because they do not re-allocate resources, the learning approaches
actually save energy, but do so by missing the performance target..

\begin{figure*}[htp!]
  \input{img/multi-app-performance.tex}
   \vskip -1em
  \caption{Multiple applications MAPE}
  \label{fig:multi-perf}
\end{figure*}

\begin{figure*}[t]
  \input{img/multi-app-energy.tex}
   \vskip -1em
  \caption{Multiple applications Energy}
  \label{fig:multi-energy}
\end{figure*}

To demonstrate \SYSTEM{} in this dynamic environment we look at the
specific example of \texttt{bodytrack} with a 70\% target.  The time
series data for bodytrack is shown in \figref{fig:bodytrack-multiapp}.
The figures show time (measured in frames) on the x-axis and
performance/power on the y-axes.  Performance is normalized to the
target.  There is a curve for each of LEO, POET, and \SYSTEM{}.

\begin{figure}[t]
  \input{img/bodytrack-multiapp.tex}
   \vskip -1em
  \caption{Bodytrack multiapp}
  \label{fig:bodytrack-multiapp}
\end{figure}

This small example clearly illustrates the benefits of \SYSTEM{}
compared to prior approaches that use only control or learning.  There
are two key regions in the figures, the times before the second
application starts (on the left of the vertical dashed line) and the
times after (on the right).  Before the second application starts (at
the dashed line), both LEO and \SYSTEM{} do a good job of tracking the
target and keeping energy low.  In contrast, POET produces oscillating
performance (and thus power) because it has a bad model of LITTLE core
performance causing it to use the LITTLE cores too much and then too
little.  This oscillation also results in unnecessary power
consumption.  After the second application starts, POET and \SYSTEM{}
recognize the performance has changed and adjust resource usage.
\SYSTEM{} does a slightly better job of tracking the change, producing
fewer oscillations.  LEO cannot adjust to the change as it computes
the optimal configuration once at the beginning of the application.
Overall, LEO produces a MAPE of 13\%, POET's is 9\%, and \SYSTEM{}'s
is 4\%.  \SYSTEM{} produces better results than LEO because it reacts
to the change; it produces better results than POET because it has
captured the complex application-specific behavior on this system.






\PUNT{
\subsection{Power and performance estimation}
We use LEO as described in \secref{sec:framework:HBM} to estimate the
power and performance for the applications using only 10 out of 128
observations.

Our results are summarized in \figref{fig:accuracy}. LEO is 13.4\%
better than online and 19.1\% better than offline in terms of
performance estimation and LEO is 3.7\% better than online and 1.5\%
better than offline in terms of power estimation, on average over all
the benchmarks.
%Infact, LEO uniformly performs well for all applications with the worst accuracy being YY as compared to offline and online algorithm.


\begin{figure}[t]
  \input[width=\columnwidth]{img/accuracy-performance.tex}
   \vskip -1em
  \caption{Accuracy performance}
  \label{fig:accuracy}
\end{figure}
}

\subsubsection{Phase Change}
In this experiment we demonstrate that \SYSTEM{} allows applications
to operate well by changing resource allotment when the input varies
with time. In \figref{fig:x264-phase-change} we see \texttt{x264}, a
video encoder application with 2 different phases, where the phase
change occurs at the $180^{th}$ frame. The first scene is difficult
and the second one becomes significantly easier. In the first phase,
\SYSTEM{} and the baseline LEO-P2I seem to have lower MAPE as compared
to POET.  At the same time, these two algorithms seem to consume less
energy than POET indicating that POET is operating at a configuration
not on the Pareto frontier of power and performance.  When we change
phase, \SYSTEM{} and LEO-P2I both algorithms are still able to meet
the performance target with far less fluctuations as compared to POET.
But, \SYSTEM{} provides better energy savings as compared to LEO-P2I.

\begin{figure}[t]
  \input{img/x264-phases2.tex}
   \vskip -1em
  \caption{X264 phase change}
  \label{fig:x264-phase-change}
\end{figure}
\PUNT{
\PUNT{
\begin{figure}[t]
  \input{img/ferret-multiapp.tex}
   \vskip -1em
  \caption{ferret multiapp}
  \label{fig:multi-energy}
\end{figure}
}
}
%}


\subsection{Sensitivity to the Measured Samples}
We examine \SYSTEM{}'s sensitivity to sample size.  We vary the number
of samples taken online and show how it affects the accuracy of the
learned models.  We note that this is simply the HBM's accuracy in
producing the model used by the controller.  This number is
significant, because we do not want to take a large number of samples
of new applications, if we can avoid it.

\figref{fig:sensitivity} shows the results and compares \SYSTEM{}'s
accuracy to the online approach for learning both performance (top)
and power (bottom).  The figure shows sample size on the x-axis and
accuracy on the y-axis.  \SYSTEM{}'s HBM initially performs as well as
the Offline approach and as sample size increases, the accuracy
uniformly improves and reaches greater than 90\% with around 20
samples. On the other hand, the online approach needs at least 7
samples so that the design matrix for the polynomial regression has
more samples than variables.  As we get more samples the accuracy for
the online model improves but still does not meet \SYSTEM{}'s accuracy
for the same number of samples.

\begin{figure}[t]
  \input{img/sample-accuracy.tex}
   \vskip -1em
  \caption{Estimation accuracy versus sample size.}
  \label{fig:sensitivity}
\end{figure}

%\PUNT{

\subsection{Overhead}
The main overhead of \SYSTEM{} is due to sampling where the
applications need to run through a few configurations before \SYSTEM{}
can reliably estimate the entire power and performance frontier. We
argue that the sampling cost can be distributed across devices by
asking each of them to contribute samples for estimation. Once the
sampling phase is over, the HBM is quite fast and can generate an
estimate as fast as 500 ms which is significantly smaller than the
time required for sampling the applications. 


The LCS requires only a few floating point operations to execute, plus
the table lookups in the PHT.  To evaluate the LCS' overhead, we time
1000 iteration of the controller.  We find that it is under 2
microseconds, which is significantly faster than we can change any
resource allocation on our system.  We conclude that the LCS has
negligible impact on performance and energy consumption of the
controlled device.











