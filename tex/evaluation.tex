\section{Experimental Setup}
\PUNT{\begin{figure}[t]
  \subfloat[]
  {
    \includegraphics[width=.22\textwidth]{figures/odroid.png}
    \label{fig:odroid}
  }
  \subfloat[]
  {
    \includegraphics[width=.22\textwidth]{figures/odroidall.png}
    \label{fig:odroid_all}
  }
 \caption{ODROID-XU3 boards used in the evaluation.}
 \label{fig:odroidall}
\end{figure}
}

\subsection{Platform and Benchmarks}
We run applications on an ODROID-XU3 with a Samsung Exynos 5 Octa
processor (an ARM big.LITTLE architecture), running Ubuntu 14.04. The
4 big cores support 19 clockspeeds, the 4 LITTLE ones have 13.  An
on-board power meter updated at 0.25s intervals captures core, GPU,
network, and memory. We allocate cores using thread affinity and set
speeds using \texttt{cpufrequtils}.  The ODROID has no screen, but
recent trends in mobile/embedded processor design and workloads have
seen processor power become the dominant factor in energy consumption
\cite{HPCA2016}.  We run the learners on an Intel server with E5-2690
processors.  The ODROID and the server are connected with Gigabit
Ethernet.

We use 12 benchmarks representing embedded and mobile sensor
processing.  These include video encoding (\texttt{x264})), video
analysis (\texttt{bodytrack}), image similarity search
(\texttt{ferret}), and animation (\texttt{facesim}) from PARSEC
\cite{parsec}; medical imaging (\texttt{heartwall},
\texttt{leukocyte}), image processing (\texttt{srad}), and machine
learning (\texttt{kmeans}) from Rodinia \cite{rodinia}; security
(\texttt{sha}) from ParMiBench \cite{parmibench}; memory intensive
processing (\texttt{stream}) \cite{stream}; and synthetic aperture
radar (\texttt{radar}) \cite{radar}.

\figref{fig:application_variety} shows the variety of workloads
indicated by the \emph{lack-of-fit}---the absence of correlation
between frequency and performance.  Applications with high lack-of-fit
do not speed up with increasing frequency---typical of memory bound
applications. Applications with low lack-of-fit increase performance
with increasing clock speed.  Applications with intermediate
lack-of-fit tend to improve with increasing clock speed up to a point
and then stop.  Each application has an outer loop which processes one
input (\eg{} a point for \texttt{kmeans} or a frame for
\texttt{x264}). The application signals the completion of an input
using a standard API \cite{icac2010heartbeats}.  Performance
requirements are specified as latencies for these inputs.

\begin{figure}[t]
  \input{img/mem-compute.tex}
   \vskip -1em
   \caption{\emph{Lack-of-fit} for performance vs clock-speed. Lower
     lack-of-fit indicates a more compute-bound application, higher
     values indicate a memory-bound one.}
  \label{fig:application_variety}
\end{figure}


\subsection{Evaluation Metrics}
%\SYSTEM{} uses the model for power and performance and its controller
%actively uses this knowledge to meet the performance target.
For each application, we measure its worst-case execution time (wcet)
running without management; \ie{} the highest latency for any input.
We set a latency goal---or \emph{deadline}---for each input equal to
its wcet; the standard approach for ensuring real-time latency
guarantees or maximum responsiveness \cite{book}. We quantify
performance reliability by measuring the missed deadlines. If the
application processes $n$ total inputs and $m$ exceeded the target
latency the deadline misses are:
\begin{equation}
deadline\, misses = 100\% \cdot \frac{m}{n}.
\end{equation}
\begin{figure*}[t]
\centering
 \subfloat[Single-App]
  {
   \input{img/single-app-summary2-v4.tex}  \label{fig:single-sum}
  }
   %\vskip -.8em
 \subfloat[Mulit-App]
  {
    \input{img/multi-app-summary2-v4.tex}  \label{fig:multi-sum}
  }
   \vskip -.5em
  %\vskip -.8em
  \caption{\footnotesize Summary data for (a) single- and (b) multi-app scenarios.
    The top row shows deadline misses, the bottom energy consumption.}
\end{figure*}
We evaluate energy savings by running every application in every
resource configuration and recording performance and power for every
input.  By post-processing this data we determine the minimal energy
resource configuration that meets the latency for each input. To
compare across applications, we normalize energy:
\begin{equation}
  normalized\,energy = 100\% . \left( \frac{e_{measured}}{e_{optimal}} - 1 \right)
\end{equation}
where $e_{measured}$ is measured energy and $e_{optimal}$ is the
optimal energy. We subtract 1, so that this metric shows the
percentage of energy over optimal.

\subsection{Points of Comparison}
We compare to existing learning and control approaches:
\begin{enumerate}[leftmargin=1em]
\item \textit{Race-to-idle}: This well-known heuristic allocates all
  resources to the application to complete each input as fast as
  possible, then idles until the next input is available
  \cite{kim-cpsna,powerslope,LeSueur11}.  This heuristic is a standard
  way to meet hard deadlines, but it requires conservative resource
  allocation \cite{book}.
\item \textit{PID-Control}: a standard single-input (performance),
  multiple-output (big/LITTLE core counts and speeds)
  prop\-ortional-integral-controller representative of several that
  have been proposed for computer resource management
  \cite{Hellerstein2004a,METE}.  This controller is tuned to provide
  the best average case behavior across all applications and targets.
\item \textit{Online}: measures a few sample configurations then
  performs polynomial multivariate regression to estimate unobserved
  configurations' behavior \cite{LEO,Li2006,Ponamarev}.
\item \textit{Offline}: does not observe the current
  application---instead using previously observed applications to
  estimate power and performance as a linear regression
  \cite{PUPiL,LeeBrooks2006,CPR,reddiHPCA2013}.
\item \textit{Netflix}: a matrix completion algorithm for the
  Netflix challenge. Variations of this approach allocate
  heterogeneous resources in data centers \cite{Paragon,quasar}.
\item \textit{HBM}: a hierarchical Bayesian learner previously used
  to allocate resources to meet performance goals with minimal energy
  in server systems \cite{LEO}.
\item \textit{Adaptive-Control}: a state-of-the-art, adaptive
  controller that meets application performance with minimal energy
  \cite{POET}.  This approach requires a user-specified model relating
  resource configuration to performance and power.  For this paper, we
  use the \emph{Offline} learner's model.  \PUNT{A recent study
    comparing ML and control techniques for resource allocation showed
    that there was no single best approach, but adaptive control had
    the best average behavior \cite{TAAS}.}
\end{enumerate}
We compare the above baselines to:
\begin{enumerate}[leftmargin=1em]
\item \textit{CALOREE-NoPole}: uses the HBM learner, but sets the pole
  to 0, which shows the importance of incorporating the learned
  variance into control. All other versions of \SYSTEM{} set the pole
  according to \secref{guarantees}.
\item \textit{CALOREE-online}: uses the online learner.
\item \textit{CALOREE-Netflix}: uses the Netflix learner.
\item \textit{CALOREE-HBM}: uses the HBM learner.
\end{enumerate}We use leave-one-out cross validation. To test
application $x$, we train the learners on all other applications, 
then test on $x$.



\section{Experimental Evaluation}


\PUNT{
\begin{figure}[t]
\centering
  \input{img/single-app-summary2-v4.tex}
   \vskip -1.0em
  \caption{Summary data for single-app scenario.}
  \label{fig:single-sum}
\end{figure}
}
\subsection{Performance and Energy for Single App}
%Setup

\figref{fig:single-sum} summarizes the average error across all
targets for the single application scenario. The figure shows deadline
misses in the top chart and energy over optimal in the bottom.  The
dots show the average, while the error bars show the minimum and
maximum values.

Race-to-idle meets all deadlines, but its conservative resource
allocation has the highest average energy consumption. Among the prior
approaches HBM has the lowest average deadline misses (9\%) and lowest
energy (20\% more than optimal). \SYSTEM{} with no pole misses 15\% of
all deadlines, which is worse than prior approaches. Note that all
prior approaches---other than racing---have at least one application
that misses all deadlines. In many cases these approaches are close to
the latency (within 10\%), but not close enough to deliver reliable
performance.


\begin{figure*}[t]
 \captionsetup[subfigure]{labelformat=empty}
  \subfloat[]
  {
    \input{img/single-app-performance-small.tex}
  }
  \\
  \vskip -1.5em
  \subfloat[]
  {
    \input{img/single-app-energy-small.tex}
  }
  \vskip -1.2em
  \caption{Comparison of application performance error and energy for single application scenario.}
 \label{fig:single-all}
\end{figure*}

\begin{figure*}[t]
 \captionsetup[subfigure]{labelformat=empty}
  \subfloat[]
  {
    \input{img/multi-app-performance-small.tex}
  }
  \\
  \vskip -1.5em
  \subfloat[]
  {
    \input{img/multi-app-energy-small.tex}
  }
  \vskip -1.2em
  \label{fig:multi}
  \caption{Comparison of application performance error and energy for multiple
  application scenario. }
 \label{fig:multi-all}
\end{figure*}


When \SYSTEM{} adaptively tune its pole, the results greatly improve.
The best combination is \SYSTEM{}-HBM, which averages 6.0\% missed
deadlines, while consuming just 4.3\% more energy than optimal.  Thus,
\SYSTEM{}-HBM reduces average deadline misses by 65\% and energy
consumption by 13\% compared to the best prior approach. The error
bars on the \SYSTEM{}-HBM approach demonstrate that it is the only
approach---besides racing---that handles every test application; all
others see at least 100\% deadline misses for one test case. Yet,
\SYSTEM{}-HBM reduces energy consumption by 27\% compared to
race-to-idle. The energy savings comes because most inputs are not
worst case, leaving slack for smart resource allocators to
save energy.  \emph{Among many smart approaches \SYSTEM{}-HBM provides
  highly reliable performance with very low energy.}

\figref{fig:single-all} presents a detailed, per-application
comparison between \SYSTEM{}-HBM and selected prior approaches which
have performed well in other scenarios: race-to-idle, Netflix, HBM,
and adaptive control. Other data has been omitted for space.  The
benchmarks are shown on the x-axis; the y-axis shows the number of
deadline misses and the normalized energy, respectively.

\subsection{Performance and Energy for Multiple Apps}

We again launch each benchmark with a goal of meeting its worst case
latency.  Halfway through execution, we start another application
randomly drawn from our benchmark set---bound to one big core---which
interferes with the original application.  Delivering the required
latency tests the ability to react to environmental changes.

\PUNT{
\begin{figure}[t]
\centering
  \input{img/multi-app-summary2-v4.tex}
   \vskip -.8em
  \caption{Summary data for multi-app scenario.}
  \label{fig:multi-sum}
\end{figure}
}

\figref{fig:multi-sum} shows the average number of deadline misses and
energy over optimal for all approaches.  Some targets are unachievable
for some applications; specifically, \texttt{bodytrack},
\texttt{heartwall}, and \texttt{sha}. Due to these unachievable
targets, both optimal and race-to-idle show some deadline misses.
Race-to-idle misses more deadlines than optimal because it cannot make
use of LITTLE cores to do some work, it simply continues using all big
cores despite the degraded performance due to the second application.
Most approaches do badly in this scenario---even adaptive control has
40\% deadline misses.  \SYSTEM{}-HBM produces the lowest deadline
misses with an average of 20\%, which is only 2 points more than
optimal.  It also produces the lowest energy, just 6\% more than
optimal. \figref{fig:multi-all} shows the detailed results.

\subsection{Adapting to Phase Changes}
\begin{figure}[!h]
  \input{img/x264-phases.tex}
   \vskip -.5em
  \caption{Controlling \texttt{x264} through scene changes.  }
  \label{fig:x264-phase-change}
\end{figure}

% \begin{figure}[!h]
%   \input{img/estimation_config.tex}
%    \vskip -.5em
%    \caption{Performance and power prediction for applications across
%      different resource allocations.}
%   \label{fig:estimation_config}
% \end{figure}
We compare \SYSTEM{} and Adaptive-Control reacting to input
variations.  \figref{fig:x264-phase-change} shows the \texttt{x264}
video encoder with 2 different phases caused by a scene change
occurring at the $500^{th}$ frame. The first scene is difficult and
the second one is much easier.  In the first, \SYSTEM{} is closer to
the desired performance (1 in the figure) and operates at a lower
power state compared to Adaptive-Control.  Adaptive-Control is not
operating at a configuration on the Pareto frontier of power and
performance.  When the input changes, \SYSTEM{} still meets the
performance target with fewer fluctuations compared to
Adaptive-Control.


\subsection{The Pole's Importance}
%\TODO{Add the LAVAMD contour plot?}
\PUNT{
\begin{figure}
\centering
  \subfloat[]
  {
    \includegraphics[width=.2\textwidth]{figures/lavamd2.pdf}
    \label{fig:lavamd}
  }
  \subfloat[]
  {
    \input{img/LAVA-pole.tex}
    \label{fig:lavamd-pole}
  }
  \caption{(a) LAVAMD's performance with different resources. (b) The
    pole's effects on LAVAMD's behaviors.}
  \label{fig:lavamd-is-hard}
\end{figure}
}

%\begin{wrapfigure}{r}{0.5\columnwidth}
%\includegraphics[width=.25\textwidth]{figures/lavamd.png}
%\caption{Performance of LAVAMD with differnt resources.}
%\label{fig:lavamd}
%\end{wrapfigure}
%LAVAMD has the most complicated responses to resource usage on our
%system, with multiple local optima as shown in \figref{fig:lavamd}.

\begin{wrapfigure}{r}{0.5\columnwidth} 
%\begin{figure}
%\centering
\input{img/LAVA-pole.tex}
\caption{Comparison of learned and default poles.}
\label{fig:lavamd-pole}
%\end{figure}
\end{wrapfigure}\secref{guarantees} argues that tuning the
controller to learned variance prevents oscillation and provides
probabilistic guarantees despite using noisy, learned data to control
unseen applications.  We demonstrate this empirically by showing LUD
using both CALOREE-NoPole and \SYSTEM{}-HBM.  \figref{fig:lavamd-pole}
shows time on the x-axis and normalized performance and power on the
y-axes.  CALOREE-NoPole oscillates and causes wide power fluctuations.
In contrast, \SYSTEM{} provides reliable performance and saves
tremendous energy because it does not oscillate but uses a mixture of
big and LITTLE cores to keep energy near minimal.

\subsection{Sensitivity to the Measured Samples}
We show how the number of samples affects model accuracy for the
Online, Netflix, and HBM learners.  We quantify accuracy as how close
the learner is to ground truth (found through exhaustive exploration),
with 1 meaning the learner perfectly models the real performance or
power.  Accuracy matters because the fewer the samples, the faster the
controller switches to the learner's application-specific model.

\figref{fig:sensitivity} shows the accuracy vs sample count for both
performance (top) and power (bottom).  The HBM incorporates prior
knowledge and its accuracy uniformly improves with more
samples---exceeding 0.9 after 20 samples. The Online approach needs
at least 7 samples to even generate a prediction.  As Online receives
more samples, its accuracy improves but never exceeds HBM's for the
same sample count. Netflix is very noisy for small sample sizes, but
after about 50, it is competitive with HBM.  These results not only
demonstrate the sensitivity to sample size, they show why
\SYSTEM{}-HBM achieves the best results.

\begin{figure}[t]
  \input{img/sample-accuracy-v3.tex}
   \vskip -.5em
  \caption{Estimation accuracy versus sample size.}
  \label{fig:sensitivity}
\end{figure}

%\PUNT{

\subsection{Overhead}
\SYSTEM{}'s main overhead is sampling, where the controller tests a
few configurations before \SYSTEM{} can reliably estimate the entire
power and performance frontier. The sampling cost can be distributed
across devices by asking each of them to contribute samples for
estimation. Once the sampling phase is over, the HBM generates an
estimate as fast as 500 ms, which is significantly smaller than the
time required to run any of our applications.  In the worst case
(\texttt{facesim}), the controller sends 320B of sample data to the
learner, which sends back 1KB.  In this case, the sampling overhead
and communication cost is less than 2\% of total execution time.
\SYSTEM{}'s asynchronous communication means that the controller never
waits for the learner.  For all other benchmarks it is lower, and for
most it is negligible.

The controller requires only a few floating point operations to
execute, plus the table lookups in the PHT.  To evaluate its overhead,
we time 1000 iterations.  We find that it is under 2 microseconds,
which is significantly faster than we can change any resource
allocation on our system; the controller has negligible impact on
performance and energy consumption of the controlled device.
