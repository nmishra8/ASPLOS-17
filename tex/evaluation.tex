\section{Experimental Setup}
\PUNT{\begin{figure}[t]
  \subfloat[]
  {
    \includegraphics[width=.22\textwidth]{figures/odroid.png}
    \label{fig:odroid}
  }
  \subfloat[]
  {
    \includegraphics[width=.22\textwidth]{figures/odroidall.png}
    \label{fig:odroid_all}
  }
 \caption{ODROID-XU3 boards used in the evaluation.}
 \label{fig:odroidall}
\end{figure}
}

\subsection{Platform and Benchmarks}
We use ODROID-XU3 devices with Samsung Exynos 5 Octa processors based
on the ARM big.LITTLE architecture, running Ubuntu 14.04. Each has 19
speed settings for the 4 big cores and 13 for 4 LITTLE cores.  Each
board has an on-board power meter updated at 1/4 s intervals capturing
core, GPU, network, and memory. We enforce core allocation using
thread affinity and core speeds using the \texttt{cpufrequtils}
package.  Our test platforms do not have screens, but recent trends in
mobile and embedded processor design and workloads have seen processor
power become the dominant factor in energy consumption
\cite{HPCA2016}.

We use 12 benchmarks representing computation in embedded and mobile
sensor processing.  These include video encoding (\texttt{x264}),
image similarity search (\cite{parsec}), video analysis
(\texttt{bodytrack}), and graphics generation from PARSEC
\cite{parsec}; medical imaging (\texttt{heartwall},
\texttt{leucocyte}), image processing (\texttt{srad}), and machine
learning (\texttt{kmeans}) from Rodinia \cite{rodinia}; security
(\texttt{sha}) from ParMiBench \cite{parmibench}; memory intensive
processing (\texttt{stream}) \cite{stream}; and a synthetic aperature
radar front end (\texttt{radar}) \cite{radar}.

\figref{fig:application_variety} shows the variety of workloads
indicated by the \emph{lack-of-fit}, or the absence of correlation
between frequency and performance.  Applications with high lack-of-fit
do not speed up with increasing frequency---typical of memory bound
applications. Applications with low lack-of-fit increase performance
with increasing clock speed \cite{powerslope}.  Applications with
intermediate lack-of-fit tend to improve with increasing clock speed
up to a point and then stop.  Each application has an outer loop which
processes one unit in a data stream (\eg{} a point for \texttt{kmeans}
or a frame for \texttt{x264}). The application signals the completion
of processing a stream element using a standard API
\cite{icac2010heartbeats}.  Performance targets are specified as
application-specific latencies for these inputs.

\begin{figure}[t]
  \input{img/mem-compute.tex}
   \vskip -1em
   \caption{\emph{Lack-of-fit} for performance vs clock-speed. Lower
     lack-of-fit indicates a more compute-bound application, higher
     values indicate a memory-bound one.}
  \label{fig:application_variety}
\end{figure}


\subsection{Evaluation Metrics}
%\SYSTEM{} uses the model for power and performance and its controller
%actively uses this knowledge to meet the performance target.
For each application, we measure its worst-case execution time (wcet)
running without management; \ie{} the highest latency for any input.
We set a latency goal--- or \emph{deadline}---for each application's
input equal to its wcet; the standard approach for ensuring real-time
latency guarantees or maximum responsiveness \cite{book}. We quantify
performance reliability by measuring the missed deadlines. If the
application processes $n$ total inputs and $m$ of those took longer
than the target latency the deadline misses are:
\begin{equation}
deadline\, misses = 100\% \cdot \frac{m}{n}.
\end{equation}
\begin{figure*}[t]
\centering
 \subfloat[Single-App]
  {
   \input{img/single-app-summary2-v4.tex}  \label{fig:single-sum}
  }
   %\vskip -.8em
 \subfloat[Mulit-App]
  {
    \input{img/multi-app-summary2-v4.tex}  \label{fig:multi-sum}
  }
   \vskip -.5em
  %\vskip -.8em
  \caption{\footnotesize Summary data for (a) single- and (b) multi-app scenarios.
    The top row shows deadline misses, the bottom energy consumption.}
\end{figure*}
We evaluate energy savings by running every application in every
resource configuration and recording performance and power for every
input.  By post-processing this data we determine the minimal energy
resource configuration that meets the latency for each input. To
compare across applications, we normalize energy:
\begin{equation}
  normalized\,energy = 100\% . \left( \frac{e_{measured}}{e_{optimal}} - 1 \right)
\end{equation}
where $e_{measured}$ is measured energy and $e_{optimal}$ is the
optimal energy. We subtract 1, so that this metric shows the
percentage of energy over optimal.

\subsection{Points of Comparison}
We compare to existing learning and control approaches:
\begin{enumerate}[leftmargin=1em]
\item \textit{Race-to-idle}: This well-known heuristic allocates all
  resources to the application to complete each input as fast as
  possible, then idles until the next input is available
  \cite{kim-cpsna,powerslope,LeSueur11}.  This heuristic is a standard
  way to meet hard deadlines, but it requires conservative resource
  allocation \cite{book}.
\item \textit{PID-Control}: a standard single-input (performance),
  multiple-output (big/LITTLE core counts and speeds)
  prop\-ortional-integral-controller representative of several that
  have been proposed for computer resource management
  \cite{Hellerstein2004a,METE}.  This controller is tuned to provide
  the best average case behavior across all applications and targets.
\item \textit{Online}: measures a few sample configurations then
  performs polynomial multivariate regression to estimate unobserved
  configurations' behavior \cite{LEO,Li2006,Ponamarev}.
\item \textit{Offline}: does not observe the current
  application---instead using previously observed applications to
  estimate power and performance as a linear regression
  \cite{PUPiL,LeeBrooks2006,CPR,reddiHPCA2013}.
\item \textit{Netflix}: a matrix completion algorithm for the
  Netflix challenge. Variations of this approach allocate
  heterogeneous resources in data centers \cite{Paragon,quasar}.
\item \textit{HBM}: a hierarchical Bayesian learner previously used
  to allocate resources to meet performance goals with minimal energy
  in server systems \cite{LEO}.
\item \textit{Adaptive-Control}: a state-of-the-art, adaptive
  controller that meets application performance with minimal energy
  \cite{POET}.  This approach requires a user-specified model relating
  resource configuration to performance and power.  For this paper, we
  use the \emph{Offline} learner's model.  \PUNT{A recent study
    comparing ML and control techniques for resource allocation showed
    that there was no single best approach, but adaptive control had
    the best average behavior \cite{TAAS}.}
\end{enumerate}
We compare the above baselines to:
\begin{enumerate}[leftmargin=1em]
\item \textit{CALOREE-NoPole}: uses the HBM learner, but sets the pole
  to 0, which shows the importance of incorporating the learned
  variance into control. All other versions of \SYSTEM{} set the pole
  according to \secref{guarantees}.
\item \textit{CALOREE-online}: uses the online learner.
\item \textit{CALOREE-Netflix}: uses the Netflix learner.
\item \textit{CALOREE-HBM}: uses the HBM learner.
\end{enumerate}We use leave-one-out cross validation: to test
application $x$, we form a set of all other applications, train the
learners, and then test on $x$.



\section{Experimental Evaluation}


\PUNT{
\begin{figure}[t]
\centering
  \input{img/single-app-summary2-v4.tex}
   \vskip -1.0em
  \caption{Summary data for single-app scenario.}
  \label{fig:single-sum}
\end{figure}
}
\subsection{Performance and Energy for Single App}
%Setup

\figref{fig:single-sum} represents the summary results as an average
error across all targets for the single application scenario. This
figure shows two charts with the percentage of deadline misses in the
top chart and the energy over optimal in the bottom.  The dots show
the average for each technique, while the error bars show the minimum
and maximum values.

Race-to-idle meets all deadlines, but its conservative resource
allocation has the highest average energy consumption. Among the prior
approaches The HBM has the lowest average deadline misses (9\%) and
lowest energy (20\% more than optimal). \SYSTEM{} with no pole misses
15\% of all deadlines, which is worse than prior approaches.


\begin{figure*}[t]
 \captionsetup[subfigure]{labelformat=empty}
  \subfloat[]
  {
    \input{img/single-app-performance-small.tex}
  }
  \\
  \vskip -1.5em
  \subfloat[]
  {
    \input{img/single-app-energy-small.tex}
  }
  \vskip -1.2em
  \caption{Comparison of application performance error and energy for single application scenario.}
 \label{fig:single-all}
\end{figure*}

\begin{figure*}[t]
 \captionsetup[subfigure]{labelformat=empty}
  \subfloat[]
  {
    \input{img/multi-app-performance-small.tex}
  }
  \\
  \vskip -1.5em
  \subfloat[]
  {
    \input{img/multi-app-energy-small.tex}
  }
  \vskip -1.2em
  \label{fig:multi}
  \caption{Comparison of application performance error and energy for multiple
  application scenario. }
 \label{fig:multi-all}
\end{figure*}


When we allow \SYSTEM{} to adaptively tune its pole, however, we see
greatly improved results.  The best combination is \SYSTEM{} with the
HBM, which misses only 6.0\% of deadlines on average, while consuming
just 4.3\% more energy than optimal.  Thus, \SYSTEM{} can reduce
deadline misses by 65\% and energy consumption by 13\% compared to the
best prior approach. The error bars on the \SYSTEM{}-HBM approach
demonstrate that it is the only approach besides race-to-idle that can
handle every test application; all others see at least 100\% deadline
misses for one test case. Yet, \SYSTEM{}-HBM reduces energy
consumption by 27\% compared to race-to-idle.

\figref{fig:single-all} presents a detailed, per-application
comparison between \SYSTEM{}-HBM and selected prior approaches which
have performed well in other scenarios: race-to-idle, netflix, HBM,
and adaptive control---other data has been omitted for space.  The
benchmarks are shown on the x-axis; the y-axis shows the number of
deadline misses and the normalized energy, respectively.

\subsection{Performance and Energy for Multiple Apps}

We again launch each benchmark with a goal of meeting the worst case
latency target.  Halfway through execution, we start another
application randomly drawn from our benchmark set---bound to one big
core---which interferes with the original application.  Delivering the
required latency in this scenario tests the ability to react to
environmental changes.

\PUNT{
\begin{figure}[t]
\centering
  \input{img/multi-app-summary2-v4.tex}
   \vskip -.8em
  \caption{Summary data for multi-app scenario.}
  \label{fig:multi-sum}
\end{figure}
}

\figref{fig:multi-sum} summarizes the results as the average number of
deadline misses and energy over optimal for all approaches.  We note
that some targets are unachievable for some applications;
specifically, \texttt{bodytrack}, \texttt{heartwall}, and
\texttt{sha}. Due to these unachievable targets, both optimal and
race-to-idle show some deadline misses. Race-to-idle misses more
deadlines than optimal because it cannot make use of LITTLE cores to
do some work, it simply continues using all big cores despite the
degraded performance due to the second application.  In fact, most
approaches do badly in this scenario---even adaptive control misses
40\% of the deadlines.  \SYSTEM{}-HBM produces the lowest deadline
misses with an average of 20\%, which is only 2 points more than
optimal.  It also produces the lowest energy, just 6\% more than
optimal. \figref{fig:multi-all} shows the detailed results.

\subsection{Adapting to Phase Changes}
\begin{figure}[!h]
  \input{img/x264-phases.tex}
   \vskip -.5em
  \caption{Controlling \texttt{x264} through scene changes.  }
  \label{fig:x264-phase-change}
\end{figure}

% \begin{figure}[!h]
%   \input{img/estimation_config.tex}
%    \vskip -.5em
%    \caption{Performance and power prediction for applications across
%      different resource allocations.}
%   \label{fig:estimation_config}
% \end{figure}
We compare \SYSTEM{} and Adaptive-Control reacting to input
variations.  \figref{fig:x264-phase-change} shows the \texttt{x264}
video encoder application with 2 different phases caused by a scene
change in the input occurring at the $500^{th}$ frame. The first scene
is difficult and the second one is much easier.  In the first,
\SYSTEM{} is closer to the desired performance (1 in the figure) and
operates at a lower power state compared to Adaptive-Control.
Adaptive-Control is not operating at a configuration on the Pareto
frontier of power and performance.  When the input changes, \SYSTEM{}
still meets the performance target with fewer fluctuations compared
to Adaptive-Control.


\subsection{The Pole's Importance}
%\TODO{Add the LAVAMD contour plot?}
\PUNT{
\begin{figure}
\centering
  \subfloat[]
  {
    \includegraphics[width=.2\textwidth]{figures/lavamd2.pdf}
    \label{fig:lavamd}
  }
  \subfloat[]
  {
    \input{img/LAVA-pole.tex}
    \label{fig:lavamd-pole}
  }
  \caption{(a) LAVAMD's performance with different resources. (b) The
    pole's effects on LAVAMD's behaviors.}
  \label{fig:lavamd-is-hard}
\end{figure}
}

%\begin{wrapfigure}{r}{0.5\columnwidth}
%\includegraphics[width=.25\textwidth]{figures/lavamd.png}
%\caption{Performance of LAVAMD with differnt resources.}
%\label{fig:lavamd}
%\end{wrapfigure}
%LAVAMD has the most complicated responses to resource usage on our
%system, with multiple local optima as shown in \figref{fig:lavamd}.

\begin{wrapfigure}{r}{0.5\columnwidth} 
%\begin{figure}
%\centering
\input{img/LAVA-pole.tex}
\caption{Comparison of learned and default poles.}
\label{fig:lavamd-pole}
%\end{figure}
\end{wrapfigure}\secref{guarantees} argues that tuning the
controller to learned variance prevents oscillation and provides
probabilistic guarantees despite using noisy, learned data to control
unseen applications.  We demonstrate this empirically by showing LUD's
single-app behavior using both CALOREE-NoPole and \SYSTEM{}-HBM.
\figref{fig:lavamd-pole} shows the results, with time on the x-axis
and normalized performance and power on the y-axes.  CALOREE-NoPole
oscillates and causes wide fluctuations in power consumption.  In
contrast, \SYSTEM{} provides reliable performance right at the target
value (normalized to 1).  \SYSTEM{} also saves tremendous energy
because it does not oscillate but uses a mixture of big and LITTLE
cores to keep energy near minimal.

\subsection{Sensitivity to the Measured Samples}
We vary the number of samples taken and show how it affects learning
accuracy for the Online, Netflix, and HBM learners.  We quantify
accuracy as how close the learner is to ground truth (found through
exhaustive exploration), with 1 meaning the learner perfectly models
the real performance or power.  Accuracy is significant because the
smaller the number of samples, the faster the controller can switch to
the learner's application-specific predictions.

\figref{fig:sensitivity} compares the Online, HBM, and Netflix
learners's accuracy for both performance (top) and power (bottom) as a
function of the number of samples. The HBM incorporates prior
knowledge and as sample size increases, its accuracy uniformly
improves, exceeding 90\% after 20 samples. The Online approach needs
at least 7 samples to even generate a prediction.  As Online receives
more samples, its accuracy improves but never exceeds HBM's for the
same number of samples. Netflix is very noisy for small sample sizes,
but after about 50, it is competitive with HBM.  These results not
only demonstrate the sensitivity to sample size, they show why
\SYSTEM{}-HBM achieves the best results.

\begin{figure}[t]
  \input{img/sample-accuracy-v3.tex}
   \vskip -.5em
  \caption{Estimation accuracy versus sample size.}
  \label{fig:sensitivity}
\end{figure}

%\PUNT{

\subsection{Overhead}
\SYSTEM{}'s main overhead is sampling, where the controller tests a
few configurations before \SYSTEM{} can reliably estimate the entire
power and performance frontier. The sampling cost can be distributed
across devices by asking each of them to contribute samples for
estimation. Once the sampling phase is over, the HBM generates an
estimate as fast as 500 ms, which is significantly smaller than the
time required to run any of our applications.  In the worst case
(\texttt{facesim}), the controller sends 160B of sample data to the
learner, which sends back 1KB.  In this case, the sampling overhead
and communication cost is less than 2\% of total execution time.
\SYSTEM{}'s asynchronous communication means that the controller never
waits for the learner.  For all other benchmarks it is lower, and for
most it is negligible.

The controller requires only a few floating point operations to
execute, plus the table lookups in the PHT.  To evaluate its overhead,
we time 1000 iterations.  We find that it is under 2 microseconds,
which is significantly faster than we can change any resource
allocation on our system; the controller has negligible impact on
performance and energy consumption of the controlled device.
